{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6795ca281c04336bc849c06eaa5e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_954205745d94452dbbe160bf08cef6f0",
              "IPY_MODEL_4e34056c9be14cd885e78bba565335a7",
              "IPY_MODEL_35a92d08199a4ebc939c689f08699ea6"
            ],
            "layout": "IPY_MODEL_1368912925fd41ec82cdd067650d2fd0"
          }
        },
        "954205745d94452dbbe160bf08cef6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373f4b91025243ad826e3a3789ac3a4b",
            "placeholder": "​",
            "style": "IPY_MODEL_1344293d47a54a62bda2d2855dd63efc",
            "value": "modules.json: 100%"
          }
        },
        "4e34056c9be14cd885e78bba565335a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e85abde34ce9411583d9a6d8f7835047",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3ae9e26d58a453ba08d4bdd5be58745",
            "value": 385
          }
        },
        "35a92d08199a4ebc939c689f08699ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5220d8a46d1a437cb41d0b79cc38973d",
            "placeholder": "​",
            "style": "IPY_MODEL_243ea69df24d444f877dc4e43e964480",
            "value": " 385/385 [00:00&lt;00:00, 18.6kB/s]"
          }
        },
        "1368912925fd41ec82cdd067650d2fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373f4b91025243ad826e3a3789ac3a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1344293d47a54a62bda2d2855dd63efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e85abde34ce9411583d9a6d8f7835047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ae9e26d58a453ba08d4bdd5be58745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5220d8a46d1a437cb41d0b79cc38973d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "243ea69df24d444f877dc4e43e964480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3699b7f269a4902900c5ccc29e994dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3105b7334b52405ba2f3f06e6fba7131",
              "IPY_MODEL_9ce887b659c54bb78cea5844dd6520fe",
              "IPY_MODEL_20d2d84c08aa4a5a994643a9b765664d"
            ],
            "layout": "IPY_MODEL_0e916adaf1ec4eda8fa0d7ee1df5ff00"
          }
        },
        "3105b7334b52405ba2f3f06e6fba7131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127c20c0824740cf865bfc10a4d535d7",
            "placeholder": "​",
            "style": "IPY_MODEL_9cba357d89834c4fb4713b559935b0ec",
            "value": "README.md: 100%"
          }
        },
        "9ce887b659c54bb78cea5844dd6520fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d64d29edd504ef599b5d88896a2c05c",
            "max": 67863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60ea7012d0c24ba19cddf4c402be16c3",
            "value": 67863
          }
        },
        "20d2d84c08aa4a5a994643a9b765664d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d286204a62e444aa9a0f176275ec4ae",
            "placeholder": "​",
            "style": "IPY_MODEL_0514870763434485a1ff2ff40d413f3b",
            "value": " 67.9k/67.9k [00:00&lt;00:00, 3.29MB/s]"
          }
        },
        "0e916adaf1ec4eda8fa0d7ee1df5ff00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "127c20c0824740cf865bfc10a4d535d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cba357d89834c4fb4713b559935b0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d64d29edd504ef599b5d88896a2c05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ea7012d0c24ba19cddf4c402be16c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d286204a62e444aa9a0f176275ec4ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0514870763434485a1ff2ff40d413f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82c4c115e6d04cb6a8fc86657202922d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f196c5e4caa4f308d8701200793c4c0",
              "IPY_MODEL_23f5d63eb57b423eb16fac508bea2351",
              "IPY_MODEL_36dc3485bb8148aeaf1beca3f43f06b5"
            ],
            "layout": "IPY_MODEL_5f903098113c44e7ba223cc78b651092"
          }
        },
        "2f196c5e4caa4f308d8701200793c4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dee26a294635435aa92c7556402171ff",
            "placeholder": "​",
            "style": "IPY_MODEL_69c09c15dee34e11b5159fcddea53871",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "23f5d63eb57b423eb16fac508bea2351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_067086bde7464b9598045cfef1bfccff",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21b72b5d9a0b42c2a4fd2b7f109200a8",
            "value": 57
          }
        },
        "36dc3485bb8148aeaf1beca3f43f06b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0937ac5f8e77472b825b4487d2cb6b14",
            "placeholder": "​",
            "style": "IPY_MODEL_a5aec4827c3b443b9013ca9a11013e0a",
            "value": " 57.0/57.0 [00:00&lt;00:00, 2.23kB/s]"
          }
        },
        "5f903098113c44e7ba223cc78b651092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee26a294635435aa92c7556402171ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c09c15dee34e11b5159fcddea53871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "067086bde7464b9598045cfef1bfccff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b72b5d9a0b42c2a4fd2b7f109200a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0937ac5f8e77472b825b4487d2cb6b14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5aec4827c3b443b9013ca9a11013e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07f26ef774e84a31a801bff7f925720e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7af29169e5ff4823b57e178942d62dab",
              "IPY_MODEL_db975fd013a6417d915ed12316818829",
              "IPY_MODEL_da7767574d5949f58b1a70d6d1ed046c"
            ],
            "layout": "IPY_MODEL_28917f2378fb40b5b5517c346a83cd85"
          }
        },
        "7af29169e5ff4823b57e178942d62dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b98b8411b004781ab887cdf43f9153d",
            "placeholder": "​",
            "style": "IPY_MODEL_92c3e0f091ee428dbece89130fe7f217",
            "value": "config.json: 100%"
          }
        },
        "db975fd013a6417d915ed12316818829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4568c95ae74fc4b737e07bc303e342",
            "max": 619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb405200b891483498048973e73f11f0",
            "value": 619
          }
        },
        "da7767574d5949f58b1a70d6d1ed046c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34d20b84780d4c379c8df8a75f9fdec2",
            "placeholder": "​",
            "style": "IPY_MODEL_a448e189940142bab64c7c642a0837a4",
            "value": " 619/619 [00:00&lt;00:00, 29.1kB/s]"
          }
        },
        "28917f2378fb40b5b5517c346a83cd85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b98b8411b004781ab887cdf43f9153d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92c3e0f091ee428dbece89130fe7f217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f4568c95ae74fc4b737e07bc303e342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb405200b891483498048973e73f11f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34d20b84780d4c379c8df8a75f9fdec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a448e189940142bab64c7c642a0837a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dee905d09a23418b9a0930d3e8be2a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03fc51ed5ace421c8753d9b5c577b39d",
              "IPY_MODEL_2b1a52e9d2674f8394de7a463d33b9c6",
              "IPY_MODEL_33a9343e4f024d92afdb540a1b9cd97f"
            ],
            "layout": "IPY_MODEL_fd0e4eb5ddaf457eb5a804381a95bb4c"
          }
        },
        "03fc51ed5ace421c8753d9b5c577b39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d1c901431741bcb3a1463e1a0a40a4",
            "placeholder": "​",
            "style": "IPY_MODEL_db233e7c38e4435b8ef5d9849270ba11",
            "value": "model.safetensors: 100%"
          }
        },
        "2b1a52e9d2674f8394de7a463d33b9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a3053e26a9045c188b39172445db0ca",
            "max": 670332568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d0460ae1e3043b69a61b4f29a55e625",
            "value": 670332568
          }
        },
        "33a9343e4f024d92afdb540a1b9cd97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f59c22f2024efdad1b98e9994cd96a",
            "placeholder": "​",
            "style": "IPY_MODEL_82f1948b41b04bb082c338283c23c74e",
            "value": " 670M/670M [00:16&lt;00:00, 56.9MB/s]"
          }
        },
        "fd0e4eb5ddaf457eb5a804381a95bb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d1c901431741bcb3a1463e1a0a40a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db233e7c38e4435b8ef5d9849270ba11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a3053e26a9045c188b39172445db0ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d0460ae1e3043b69a61b4f29a55e625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72f59c22f2024efdad1b98e9994cd96a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f1948b41b04bb082c338283c23c74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b7b8ade66594cd7af7abd64ce3ddc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41dca18c157b4600a6ae64e66f1c331e",
              "IPY_MODEL_69df73d7db374f0f8dd5debb76b3d7e1",
              "IPY_MODEL_8a142a430fce4fb9a9a20dac0118dc66"
            ],
            "layout": "IPY_MODEL_67bd389a36044d9c947a39aad6de948a"
          }
        },
        "41dca18c157b4600a6ae64e66f1c331e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf23a831dd843dc90064149fd52c8b7",
            "placeholder": "​",
            "style": "IPY_MODEL_b718d550df5043058c950a8b007eebfe",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "69df73d7db374f0f8dd5debb76b3d7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb4c24f28cf748abba0b192676c95960",
            "max": 342,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9af9f09d9c214ab499333073358078b8",
            "value": 342
          }
        },
        "8a142a430fce4fb9a9a20dac0118dc66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30d1cfccf9794d5fb63abb4ff881372c",
            "placeholder": "​",
            "style": "IPY_MODEL_f662a0be9508413f8e43eb62dede4e2f",
            "value": " 342/342 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "67bd389a36044d9c947a39aad6de948a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf23a831dd843dc90064149fd52c8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b718d550df5043058c950a8b007eebfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb4c24f28cf748abba0b192676c95960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af9f09d9c214ab499333073358078b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30d1cfccf9794d5fb63abb4ff881372c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f662a0be9508413f8e43eb62dede4e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad446d725b344d92ba3aaed2a28e1196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f6d077d69534777b6514bc7adc61a53",
              "IPY_MODEL_8c108e5dab2249e7a728c75d34d50d1c",
              "IPY_MODEL_5c69977484ff463ea1ecfd091581db35"
            ],
            "layout": "IPY_MODEL_a5e02dec50164e7c8c8f91f16d1ab103"
          }
        },
        "7f6d077d69534777b6514bc7adc61a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc4fc9a691314c0eb29fb66bf8bd1c29",
            "placeholder": "​",
            "style": "IPY_MODEL_d853823e15774f099b2c7a4ba6a6bcc7",
            "value": "vocab.txt: 100%"
          }
        },
        "8c108e5dab2249e7a728c75d34d50d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a23ad0612334099ac863350e490158a",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae0cfaf4c8414a5987e4ac739d6d4a94",
            "value": 231508
          }
        },
        "5c69977484ff463ea1ecfd091581db35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e526c064324623849edae03ba80e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_ecda8314ba2c4cb7a30e7601aebc17a6",
            "value": " 232k/232k [00:00&lt;00:00, 8.43MB/s]"
          }
        },
        "a5e02dec50164e7c8c8f91f16d1ab103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc4fc9a691314c0eb29fb66bf8bd1c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d853823e15774f099b2c7a4ba6a6bcc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a23ad0612334099ac863350e490158a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae0cfaf4c8414a5987e4ac739d6d4a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4e526c064324623849edae03ba80e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecda8314ba2c4cb7a30e7601aebc17a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e0f53e53e446519923441940614852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41b23af96644426da7cf327329fcbcc3",
              "IPY_MODEL_89570f382079458aacb988c56f6b517c",
              "IPY_MODEL_7b52fb85164941fbaf871f493638aba7"
            ],
            "layout": "IPY_MODEL_dcf3ece4983645bead0865b32d506d56"
          }
        },
        "41b23af96644426da7cf327329fcbcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8573417e2ce743db8de78a8965541163",
            "placeholder": "​",
            "style": "IPY_MODEL_2eb5874fdbad47368014b4acacba82e5",
            "value": "tokenizer.json: 100%"
          }
        },
        "89570f382079458aacb988c56f6b517c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81df18a6e3c43b98b9ff6da220f4bcd",
            "max": 711661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3cda969ae2b41dca2fc1e78328d49b3",
            "value": 711661
          }
        },
        "7b52fb85164941fbaf871f493638aba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c4f21595ca4fe7b23716642167269e",
            "placeholder": "​",
            "style": "IPY_MODEL_b990b6d841c747158fb8c26524cd265d",
            "value": " 712k/712k [00:00&lt;00:00, 28.0MB/s]"
          }
        },
        "dcf3ece4983645bead0865b32d506d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8573417e2ce743db8de78a8965541163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb5874fdbad47368014b4acacba82e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a81df18a6e3c43b98b9ff6da220f4bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3cda969ae2b41dca2fc1e78328d49b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0c4f21595ca4fe7b23716642167269e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b990b6d841c747158fb8c26524cd265d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4157e28438464d1d96a60c617c1b7fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4c950b1526d40c2b3f14dc7d677037f",
              "IPY_MODEL_41c2806ce7ca4546ab5f7d6357ebd6aa",
              "IPY_MODEL_d6a58b145587491e86a8a4205d3f2dec"
            ],
            "layout": "IPY_MODEL_eab1ea524d58474d9336b244050a9e1b"
          }
        },
        "e4c950b1526d40c2b3f14dc7d677037f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0fe74d2c7dc45bd8ea56229a2053a21",
            "placeholder": "​",
            "style": "IPY_MODEL_2753e05e17844aaaa2842a742017c877",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "41c2806ce7ca4546ab5f7d6357ebd6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f7271d056604a228a09904c3076e3a2",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd88ad3e5cf24cd3b6cae39700a945ea",
            "value": 125
          }
        },
        "d6a58b145587491e86a8a4205d3f2dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c37a56f1a27f4ee6a78fd40550b20708",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c1bb689e084bc49a632b6fb2f4ec5a",
            "value": " 125/125 [00:00&lt;00:00, 5.82kB/s]"
          }
        },
        "eab1ea524d58474d9336b244050a9e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0fe74d2c7dc45bd8ea56229a2053a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2753e05e17844aaaa2842a742017c877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f7271d056604a228a09904c3076e3a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd88ad3e5cf24cd3b6cae39700a945ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c37a56f1a27f4ee6a78fd40550b20708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c1bb689e084bc49a632b6fb2f4ec5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a68ec80a7f248b9816178a57432967d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3ee13e682834a158eef56a9c43aba23",
              "IPY_MODEL_d84e9298b8724944be256cf844b59bde",
              "IPY_MODEL_f490d8abdc784a7d9be9597e402e0526"
            ],
            "layout": "IPY_MODEL_6198fdd38c6a4217a3e269404c5da6f9"
          }
        },
        "d3ee13e682834a158eef56a9c43aba23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87da87c0c5954605800acf79ea238186",
            "placeholder": "​",
            "style": "IPY_MODEL_3e79cb41295a40ffb9da01714f23b65d",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "d84e9298b8724944be256cf844b59bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7a629f1a0a46168a43c8a09b5ff8c8",
            "max": 191,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa327a67c4f5495a9cc508d124f18edd",
            "value": 191
          }
        },
        "f490d8abdc784a7d9be9597e402e0526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e9df48e9c7a4a828ca809c47fa5363e",
            "placeholder": "​",
            "style": "IPY_MODEL_cf47c3439bc3459286c9da75df5bd2f9",
            "value": " 191/191 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "6198fdd38c6a4217a3e269404c5da6f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87da87c0c5954605800acf79ea238186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e79cb41295a40ffb9da01714f23b65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f7a629f1a0a46168a43c8a09b5ff8c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa327a67c4f5495a9cc508d124f18edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e9df48e9c7a4a828ca809c47fa5363e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf47c3439bc3459286c9da75df5bd2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb\n",
        "!pip install bitsandbytes-cuda110 bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install langchain_community\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSM85jC4GTHj",
        "outputId": "c9037421-5ae1-48dd-88a2-6222dad4b197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_core-0.3.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.2-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.125-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.2 langchain-text-splitters-0.3.0 langsmith-0.1.125 orjson-3.10.7 tenacity-8.5.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.1\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.8.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.7.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.5.7-py3-none-any.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika, durationpy\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=846468c4b42b9e6e34e21f71a7abaa30d07700e3e9e5aa9666519f9118daf767\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for durationpy: filename=durationpy-0.7-py3-none-any.whl size=3462 sha256=d5eb195c0b0635bc1caef549350ba1c88b50795141571ceccd0b212637dcf10e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/52/58/701659d0f7467c85c273d8d06d8f74f2646ee7da4145ce77b5\n",
            "Successfully built pypika durationpy\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.7 coloredlogs-15.0.1 deprecated-1.2.14 durationpy-0.7 fastapi-0.115.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-31.0.0 mmh3-5.0.0 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.6.6 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.38.5 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1\n",
            "Collecting bitsandbytes-cuda110\n",
            "  Downloading bitsandbytes_cuda110-0.26.0.post2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes_cuda110-0.26.0.post2-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes-cuda110, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.3 bitsandbytes-cuda110-0.26.0.post2\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.125)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.0 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 typing-inspect-0.9.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDJTk8289qfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d12bffe-2bcf-42da-ecac-0fad102afa12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from IPython.display import Markdown, display\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "import PyPDF2\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = 'deep-leraning-sarker.pdf'"
      ],
      "metadata": {
        "id": "4GG1gGTz8mi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(pdf_path, 'rb') as file:\n",
        "      pdf_reader = PyPDF2.PdfReader(file)\n",
        "      pages = pdf_reader.pages\n",
        "      print(len(pages))\n",
        "      text = \"\"\n",
        "      for page in pages :\n",
        "\n",
        "        text+= page.extract_text()+'\\n'\n",
        "#\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yGheRIW6R27",
        "outputId": "9533cc62-b0ad-478f-cace-caf7ae6edfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ELLJfYRW-TZB",
        "outputId": "cb0824a5-72aa-4515-c9e4-b97dbfa82af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/353986944\\nDeep Learning: A Comprehensive Overview on Techniques, Taxonomy,\\nApplications and Research Directions\\nArticle \\xa0\\xa0 in\\xa0\\xa0SN Comput er Scienc e · August 2021\\nDOI: 10.1007/s42979-021-00815-1\\nCITATIONS\\n1,195READS\\n5,252\\n1 author:\\nIqbal H. Sark er\\nEdith Co wan Univ ersity\\n235 PUBLICA TIONS \\xa0\\xa0\\xa010,044  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Iqbal H. Sark er on 13 Mar ch 2024.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.\\nVol.:(0123456789)SN Computer Science (2021) 2:420 \\nhttps://doi.org/10.1007/s42979-021-00815-1\\nSN Computer Science\\nREVIEW ARTICLE\\nDeep Learning: A\\xa0Comprehensive Overview on\\xa0Techniques, Taxonomy, \\nApplications and\\xa0Research Directions\\nIqbal\\xa0H.\\xa0Sarker1,2 \\nReceived: 29 May 2021 / Accepted: 7 August 2021 / Published online: 18 August 2021 \\n© The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd 2021\\nAbstract\\nDeep learning (DL), a branch of machine learning (ML) and artificial intelligence (AI) is nowadays considered as a core \\ntechnology of today’s Fourth Industrial Revolution (4IR or Industry 4.0). Due to its learning capabilities from data, DL \\ntechnology originated from artificial neural network (ANN), has become a hot topic in the context of computing, and is \\nwidely applied in various application areas like healthcare, visual recognition, text analytics,\\xa0cybersecurity, and many more. \\nHowever, building an appropriate DL model is a challenging task, due to the dynamic nature and variations in real-world \\nproblems and data. Moreover, the lack of core understanding turns DL methods into black-box machines that hamper develop-\\nment at the standard level. This article presents a structured and comprehensive view on DL techniques including a taxonomy  \\nconsidering various types of real-world tasks like supervised or unsupervised. In our taxonomy, we take into account deep \\nnetworks for supervised or discriminative learning, unsupervised or generative learning as well as hybrid learning and \\nrelevant others. We also summarize real-world application areas where deep learning techniques can be used. Finally, we \\npoint out ten potential aspects for future generation DL modeling with research directions. Overall, this article aims to draw \\na big picture on DL modeling that can be used as a reference guide for both academia and industry professionals.\\nKeywords Deep learning\\xa0· Artificial neural network\\xa0· Artificial intelligence\\xa0· Discriminative learning\\xa0· Generative \\nlearning\\xa0· Hybrid learning\\xa0· Intelligent systems\\nIntroduction\\nIn the late 1980s, neural networks became a prevalent topic \\nin the area of Machine Learning (ML) as well as Artificial \\nIntelligence (AI), due to the invention of various efficient \\nlearning methods and network structures [52]. Multilayer \\nperceptron networks trained by “Backpropagation” type \\nalgorithms, self-organizing maps, and radial basis function \\nnetworks were such innovative methods [26, 36, 37]. While \\nneural networks are successfully used in many applications, the interest in researching this topic decreased later on. \\nAfter that, in 2006, “Deep Learning” (DL) was introduced \\nby Hinton et\\xa0al. [ 41], which was based on the concept of \\nartificial neural network (ANN). Deep learning became a \\nprominent topic after that, resulting in a rebirth in neural \\nnetwork research, hence, some times referred to as “new-\\ngeneration neural networks”. This is because deep networks, \\nwhen properly trained, have produced significant success in \\na variety of classification and regression challenges [52].\\nNowadays, DL technology is considered as one of the \\nhot topics within the area of machine learning, artificial \\nintelligence as well as data science and analytics, due to its \\nlearning capabilities from the given data. Many corporations \\nincluding Google, Microsoft, Nokia, etc., study it actively \\nas it can provide significant results in different classifica-\\ntion and regression problems and datasets [52]. In terms of \\nworking domain, DL is considered as a subset of ML and \\nAI, and thus DL can be seen as an AI function that mimics \\nthe human brain’s processing of data. The worldwide popu-\\nlarity of “Deep learning” is increasing day by day, which is \\nshown in our earlier paper [96] based on the historical data This article is part of the topical collection “Advances in \\nComputational Approaches for Artificial Intelligence, Image \\nProcessing, IoT and Cloud Applications” guest edited by Bhanu \\nPrakash K. N. and M. Shivakumar.\\n * Iqbal H. Sarker \\n msarker@swin.edu.au\\n1 Swinburne University of\\xa0Technology, Melbourne, VIC\\xa03122, \\nAustralia\\n2 Chittagong University of\\xa0Engineering & Technology, \\nChittagong\\xa04349, Bangladesh\\n SN Computer Science (2021) 2:420\\n 420 Page 2 of 20\\nSN Computer Science\\ncollected from Google trends [33]. Deep learning differs \\nfrom standard machine learning in terms of efficiency as the \\nvolume of data increases, discussed briefly in Section “ Why \\nDeep Learning in Today's Research and Applications?”. DL \\ntechnology uses multiple layers to represent the abstractions \\nof data to build computational models. While deep learning \\ntakes a long time to train a model due to a large number of \\nparameters, it takes a short amount of time to run during \\ntesting as compared to other machine learning algorithms \\n[127].\\nWhile today’s Fourth Industrial Revolution (4IR or Indus-\\ntry 4.0) is typically focusing on technology-driven “automa-\\ntion, smart and intelligent systems”, DL technology, which \\nis originated from ANN, has become one of the core tech-\\nnologies to achieve the goal [103, 114]. A typical neural \\nnetwork is mainly composed of many simple, connected pro-\\ncessing elements or processors called neurons, each of which \\ngenerates a series of real-valued activations for the target \\noutcome. Figure\\xa0 1 shows a schematic representation of the \\nmathematical model of an artificial neuron, i.e., processing \\nelement, highlighting input ( Xi ), weight (w ), bias (b ), sum-\\nmation function ( ∑ ), activation function (f ) and correspond-\\ning output signal (y ). Neural network-based DL technology \\nis now widely applied in many fields and research areas such \\nas healthcare, sentiment analysis, natural language process-\\ning, visual recognition, business intelligence, cybersecurity, \\nand many more that have been summarized in the latter part \\nof this paper.\\nAlthough DL models are successfully applied in various \\napplication areas, mentioned above, building an appropri-\\nate model of deep learning is a challenging task, due to \\nthe dynamic nature and variations of real-world problems \\nand data. Moreover, DL models are typically considered as “black-box” machines that hamper the standard develop-\\nment of deep learning research and applications. Thus for \\nclear understanding, in this paper, we present a structured \\nand comprehensive view on DL techniques considering the \\nvariations in real-world problems and tasks. To achieve \\nour goal, we briefly discuss various DL techniques and \\npresent a taxonomy by taking into account three major \\ncategories: (i) deep networks for supervised or discrimi-\\nnative learning that is utilized to provide a discrimina-\\ntive function in supervised deep learning or classifica-\\ntion applications; (ii) deep networks for unsupervised \\nor generative learning  that are used to characterize the \\nhigh-order correlation properties or features for pattern \\nanalysis or synthesis, thus can be used as preprocessing \\nfor the supervised algorithm; and (ii) deep networks for \\nhybrid learning that is an integration of both supervised \\nand unsupervised model and relevant others. We take into \\naccount such categories based on the nature and learning \\ncapabilities of different DL techniques and how they are \\nused to solve problems in real-world applications [97]. \\nMoreover, identifying key research issues and prospects \\nincluding effective data representation, new algorithm \\ndesign, data-driven hyper-parameter learning, and model \\noptimization, integrating domain knowledge, adapting \\nresource-constrained devices, etc. is one of the key targets \\nof this study, which can lead to “Future Generation DL-\\nModeling”. Thus the goal of this paper is set to assist those \\nin academia and industry as a reference guide, who want \\nto research and develop data-driven smart and intelligent \\nsystems based on DL techniques.\\nThe overall contribution of this paper is summarized as \\nfollows:\\n– This article focuses on different aspects of deep learning \\nmodeling, i.e., the learning capabilities of DL techniques \\nin different dimensions such as supervised or unsuper -\\nvised tasks, to function in an automated and intelligent \\nmanner, which can play as a core technology of today’s \\nFourth Industrial Revolution (Industry 4.0).\\n– We explore a variety of prominent DL techniques and \\npresent a taxonomy by taking into account the variations \\nin deep learning tasks and how they are used for differ -\\nent purposes. In our taxonomy, we divide the techniques \\ninto three major categories such as deep networks for \\nsupervised or discriminative learning, unsupervised or \\ngenerative learning, as well as deep networks for hybrid \\nlearning, and relevant others.\\n– We have summarized several potential real-world appli-\\ncation areas of deep learning, to assist developers as well \\nas researchers in broadening their perspectives on DL \\ntechniques. Different categories of DL techniques high-\\nlighted in our taxonomy can be used to solve various \\nissues accordingly.\\nFig. 1  Schematic representation of the mathematical model of an \\nartificial neuron (processing element), highlighting input ( Xi ), weight \\n(w), bias (b), summation function ( ∑ ), activation function (f) and out-\\nput signal (y)\\nSN Computer Science (2021) 2:420 \\n Page 3 of 20 420\\nSN Computer Science\\n– Finally, we point out and discuss ten potential aspects \\nwith research directions for future generation DL mod-\\neling in terms of conducting future research and system \\ndevelopment.\\nThis paper is organized as follows. Section “ Why Deep \\nLearning in Today's Research andApplications?” motivates \\nwhy deep learning is important to build data-driven intel-\\nligent systems. In Section“ Deep Learning Techniques and \\nApplications”, we present our DL taxonomy by taking into \\naccount the variations of deep learning tasks and how they \\nare used in solving real-world issues and briefly discuss the \\ntechniques with summarizing the potential application areas. \\nIn Section “Research Directions and Future Aspects”, we \\ndiscuss various research issues of deep learning-based mod-\\neling and highlight the promising topics for future research \\nwithin the scope of our study. Finally, Section “Concluding \\nRemarks” concludes this paper.\\nWhy Deep Learning in\\xa0Today’s Research \\nand\\xa0Applications?\\nThe main focus of today’s Fourth Industrial Revolution \\n(Industry 4.0) is typically technology-driven automation, \\nsmart and intelligent systems, in various application areas \\nincluding smart healthcare, business intelligence, smart cit-\\nies, cybersecurity intelligence, and many more [95]. Deep \\nlearning approaches have grown dramatically in terms of \\nperformance in a wide range of applications considering \\nsecurity technologies, particularly, as an excellent solution \\nfor uncovering complex architecture in high-dimensional \\ndata. Thus, DL techniques can play a key role in building \\nintelligent data-driven systems according to today’s needs, \\nbecause of their excellent learning capabilities from histori-\\ncal data. Consequently, DL can change the world as well \\nas humans’ everyday life through its automation power and \\nlearning from experience. DL technology is therefore rel-\\nevant to artificial intelligence [103], machine learning [97] \\nand data science with advanced analytics [95] that are well-\\nknown areas in computer science, particularly, today’s intel-\\nligent computing. In the following, we first discuss regarding \\nthe position of deep learning in AI, or how DL technology \\nis related to these areas of computing.\\nThe Position of\\xa0Deep Learning in\\xa0AI\\nNowadays, artificial intelligence (AI), machine learning \\n(ML), and deep learning (DL) are three popular terms that \\nare sometimes used interchangeably to describe systems or \\nsoftware that behaves intelligently. In Fig.\\xa0 2, we illustrate the \\nposition of deep Learning, comparing with machine learning \\nand artificial intelligence. According to Fig.\\xa0 2, DL is a part of ML as well as a part of the broad area AI. In general, AI \\nincorporates human behavior and intelligence to machines \\nor systems [ 103], while ML is the method to learn from data \\nor experience [97], which automates analytical model build-\\ning. DL also represents learning methods from data where \\nthe computation is done through multi-layer neural networks \\nand processing. The term “Deep” in the deep learning meth-\\nodology refers to the concept of multiple levels or stages \\nthrough which data is processed for building a data-driven \\nmodel.\\nThus, DL can be considered as one of the core technol-\\nogy of AI, a frontier for artificial intelligence, which can be \\nused for building intelligent systems and automation. More \\nimportantly, it pushes AI to a new level, termed “Smarter \\nAI”. As DL are capable of learning from data, there is a \\nstrong relation of deep learning with “Data Science” [95] as \\nwell. Typically, data science represents the entire process of \\nfinding meaning or insights in data in a particular problem \\ndomain, where DL methods can play a key role for advanced \\nanalytics and intelligent decision-making [104, 106]. Over -\\nall, we can conclude that DL technology is capable to change \\nthe current world, particularly, in terms of a powerful com-\\nputational engine and contribute to technology-driven auto -\\nmation, smart and intelligent systems accordingly, and meets \\nthe goal of Industry 4.0.\\nUnderstanding Various Forms of\\xa0Data\\nAs DL models learn from data, an in-depth understanding \\nand representation of data are important to build a data-\\ndriven intelligent system in a particular application area. In \\nthe real world, data can be in various forms, which typically \\ncan be represented as below for deep learning modeling:\\n– Sequential Data Sequential data is any kind of data \\nwhere the order matters, i,e., a set of sequences. It needs \\nto explicitly account for the sequential nature of input \\ndata while building the model. Text streams, audio frag-\\nFig. 2  An illustration of the position of deep learning (DL), compar -\\ning with machine learning (ML) and artificial intelligence (AI)\\n SN Computer Science (2021) 2:420\\n 420 Page 4 of 20\\nSN Computer Science\\nments, video clips, time-series data, are some examples \\nof sequential data.\\n– Image or 2D Data A digital image is made up of a \\nmatrix, which is a rectangular array of numbers, sym-\\nbols, or expressions arranged in rows and columns in \\na 2D array of numbers. Matrix, pixels, voxels, and bit \\ndepth are the four essential characteristics or fundamental \\nparameters of a digital image.\\n– Tabular Data A tabular dataset consists primarily of \\nrows and columns. Thus tabular datasets contain data in \\na columnar format as in a database table. Each column \\n(field) must have a name and each column may only con-\\ntain data of the defined type. Overall, it is a logical and \\nsystematic arrangement of data in the form of rows and \\ncolumns that are based on data properties or features. \\nDeep learning models can learn efficiently on tabular \\ndata and allow us to build data-driven intelligent systems.The above-discussed data forms are common in the real-\\nworld application areas of deep learning. Different cat-\\negories of DL techniques perform differently depending \\non the nature and characteristics of data, discussed briefly \\nin Section “Deep Learning Techniques and Applications” \\nwith a taxonomy presentation. However, in many real-world \\napplication areas, the standard machine learning techniques, \\nparticularly, logic-rule or tree-based techniques [93, 101] \\nperform significantly depending on the application nature. \\nFigure\\xa0 3 also shows the performance comparison of DL and \\nML modeling considering the amount of data. In the fol-\\nlowing, we highlight several cases, where deep learning is \\nuseful to solve real-world problems, according to our main \\nfocus in this paper.\\nDL Properties and\\xa0Dependencies\\nA DL model typically follows the same processing stages \\nas machine learning modeling. In Fig.\\xa0 4, we have shown a \\ndeep learning workflow to solve real-world problems, which \\nconsists of three processing steps, such as data understand-\\ning and preprocessing, DL model building, and training, \\nand validation and interpretation. However, unlike the ML \\nmodeling [98, 108], feature extraction in the DL model is \\nautomated rather than manual. K-nearest neighbor, support \\nvector machines, decision tree, random forest, naive Bayes, \\nlinear regression, association rules, k-means clustering, are \\nsome examples of machine learning techniques that are com-\\nmonly used in various application areas [97]. On the other \\nhand, the DL model includes convolution neural network, \\nrecurrent neural network, autoencoder, deep belief network, \\nand many more, discussed briefly with their potential appli-\\ncation areas in Section\\xa0 3. In the following, we discuss the \\nkey properties and dependencies of DL techniques, that are \\nFig. 3  An illustration of the performance comparison between deep \\nlearning (DL) and other machine learning (ML) algorithms, where \\nDL modeling from large amounts of data can increase the perfor -\\nmance\\nFig. 4  A typical DL workflow to solve real-world problems, which consists of three sequential stages (i) data understanding and preprocessing \\n(ii) DL model building and training (iii) validation and interpretation\\nSN Computer Science (2021) 2:420 \\n Page 5 of 20 420\\nSN Computer Science\\nneeded to take into account before started working on DL \\nmodeling for real-world applications.\\n– Data Dependencies  Deep learning is typically dependent \\non a large amount of data to build a data-driven model \\nfor a particular problem domain. The reason is that when \\nthe data volume is small, deep learning algorithms often \\nperform poorly [64 ]. In such circumstances, however, \\nthe performance of the standard machine-learning algo-\\nrithms will be improved if the specified rules are used \\n[64, 107].\\n– Hardware Dependencies The DL algorithms require \\nlarge computational operations while training a model \\nwith large datasets. As the larger the computations, the \\nmore the advantage of a GPU over a CPU, the GPU is \\nmostly used to optimize the operations efficiently. Thus, \\nto work properly with the deep learning training, GPU \\nhardware is necessary. Therefore, DL relies more on \\nhigh-performance machines with GPUs than standard \\nmachine learning methods [19, 127].\\n– Feature Engineering Process Feature engineering is the \\nprocess of extracting features (characteristics, properties, \\nand attributes) from raw data using domain knowledge. A \\nfundamental distinction between DL and other machine-\\nlearning techniques is the attempt to extract high-level \\ncharacteristics directly from data [22, 97]. Thus, DL \\ndecreases the time and effort required to construct a fea-\\nture extractor for each problem.\\n– Model Training and Execution time In general, train-\\ning a deep learning algorithm takes a long time due to a \\nlarge number of parameters in the DL algorithm; thus, \\nthe model training process takes longer. For instance, the \\nDL models can take more than one week to complete a \\ntraining session, whereas training with ML algorithms \\ntakes relatively little time, only seconds to hours [107 , \\n127]. During testing, deep learning algorithms take \\nextremely little time to run [127], when compared to \\ncertain machine learning methods.\\n– Black-box Perception and Interpretability Interpret-\\nability is an important factor when comparing DL with \\nML. It’s difficult to explain how a deep learning result \\nwas obtained, i.e., “black-box”. On the other hand, the \\nmachine-learning algorithms, particularly, rule-based \\nmachine learning techniques [97] provide explicit logic \\nrules (IF-THEN) for making decisions that are easily \\ninterpretable for humans. For instance, in our earlier \\nworks, we have presented several machines learning rule-\\nbased techniques [100, 102, 105], where the extracted \\nrules are human-understandable and easier to interpret, \\nupdate or delete according to the target applications.\\nThe most significant distinction between deep learning and \\nregular machine learning is how well it performs when data grows exponentially. An illustration of the performance \\ncomparison between DL and standard ML algorithms has \\nbeen shown in Fig.\\xa0 3, where DL modeling can increase the \\nperformance with the amount of data. Thus, DL modeling is \\nextremely useful when dealing with a large amount of data \\nbecause of its capacity to process vast amounts of features \\nto build an effective data-driven model. In terms of develop-\\ning and training DL models, it relies on parallelized matrix \\nand tensor operations as well as computing gradients and \\noptimization. Several, DL libraries and resources [30] such \\nas PyTorch [82] (with a high-level API called Lightning) and \\nTensorFlow [1 ] (which also offers Keras as a high-level API) \\noffers these core utilities including many pre-trained models, \\nas well as many other necessary functions for implementa-\\ntion and DL model building.\\nDeep Learning Techniques and\\xa0Applications\\nIn this section, we go through the various types of deep \\nneural network techniques, which typically consider sev -\\neral layers of information-processing stages in hierarchical \\nstructures to learn. A typical deep neural network contains \\nmultiple hidden layers including input and output layers. \\nFigure\\xa0 5 shows a general structure of a deep neural network \\n( hidden layer =N and N ≥ 2) comparing with a shallow \\nnetwork ( hidden layer =1 ). We also present our taxonomy \\non DL techniques based on how they are used to solve vari-\\nous problems, in this section. However, before exploring the \\ndetails of the DL techniques, it’s useful to review various \\ntypes of learning tasks such as (i) Supervised: a task-driven \\napproach that uses labeled training data, (ii) Unsupervised: \\na data-driven process that analyzes unlabeled datasets, (iii) \\nSemi-supervised: a hybridization of both the supervised and \\nunsupervised methods, and (iv) Reinforcement: an environ-\\nment driven approach, discussed briefly in our earlier paper \\n[97]. Thus, to present our taxonomy, we divide DL tech-\\nniques broadly into three major categories: (i) deep networks \\nfor supervised or discriminative learning; (ii) deep networks \\nfor unsupervised or generative learning; and (ii) deep net-\\nworks for hybrid learning combing both and relevant others, \\nas shown in Fig.\\xa0 6. In the following, we briefly discuss each \\nof these techniques that can be used to solve real-world prob-\\nlems in various application areas according to their learning \\ncapabilities.\\nDeep Networks for\\xa0Supervised or\\xa0Discriminative \\nLearning\\nThis category of DL techniques is utilized to provide a \\ndiscriminative function in supervised or classification \\napplications. Discriminative deep architectures are typi-\\ncally designed to give discriminative power for pattern \\n SN Computer Science (2021) 2:420\\n 420 Page 6 of 20\\nSN Computer Science\\nclassification by describing the posterior distributions of \\nclasses conditioned on visible data [21]. Discriminative \\narchitectures mainly include Multi-Layer Perceptron (MLP), \\nConvolutional Neural Networks (CNN or ConvNet), Recur -\\nrent Neural Networks (RNN), along with their variants. In \\nthe following, we briefly discuss these techniques.\\nMulti‑layer Perceptron (MLP)\\nMulti-layer Perceptron (MLP), a supervised learning \\napproach [83], is a type of feedforward artificial neural network (ANN). It is also known as the foundation archi-\\ntecture of deep neural networks (DNN) or deep learning. A \\ntypical MLP is a fully connected network that consists of \\nan input layer that receives input data, an output layer that \\nmakes a decision or prediction about the input signal, and \\none or more hidden layers between these two that are consid-\\nered as the network’s computational engine [ 36, 103]. The \\noutput of an MLP network is determined using a variety of \\nactivation functions, also known as transfer functions, such \\nas ReLU (Rectified Linear Unit), Tanh, Sigmoid, and Soft-\\nmax [83, 96]. To train MLP employs the most extensively \\nFig. 5  A general architecture of a a shallow network with one hidden layer and b a deep neural network with multiple hidden layers\\nFig. 6  A taxonomy of DL techniques, broadly divided into three major categories (i) deep networks for supervised or discriminative learning, \\n(ii) deep networks for unsupervised or generative learning, and (ii) deep networks for hybrid learning and relevant others\\nSN Computer Science (2021) 2:420 \\n Page 7 of 20 420\\nSN Computer Science\\nused algorithm “Backpropagation” [36], a supervised learn-\\ning technique, which is also known as the most basic build-\\ning block of a neural network. During the training process, \\nvarious optimization approaches such as Stochastic Gradi-\\nent Descent (SGD), Limited Memory BFGS (L-BFGS), and \\nAdaptive Moment Estimation (Adam) are applied. MLP \\nrequires tuning of several hyperparameters such as the num-\\nber of hidden layers, neurons, and iterations, which could \\nmake solving a complicated model computationally expen-\\nsive. However, through partial fit, MLP offers the advantage \\nof learning non-linear models in real-time or online [83].\\nConvolutional Neural Network (CNN or\\xa0ConvNet)\\nThe Convolutional Neural Network (CNN or ConvNet) [ 65] \\nis a popular discriminative deep learning architecture that \\nlearns directly from the input without the need for human \\nfeature extraction. Figure\\xa0 7 shows an example of a CNN \\nincluding multiple convolutions and pooling layers. As a \\nresult, the CNN enhances the design of traditional ANN like \\nregularized MLP networks. Each layer in CNN takes into \\naccount optimum parameters for a meaningful output as well \\nas reduces model complexity. CNN also uses a ‘dropout’ \\n[30] that can deal with the problem of over-fitting, which \\nmay occur in a traditional network.\\nCNNs are specifically intended to deal with a variety of \\n2D shapes and are thus widely employed in visual recogni-\\ntion, medical image analysis, image segmentation, natural \\nlanguage processing, and many more [65, 96]. The capa-\\nbility of automatically discovering essential features from \\nthe input without the need for human intervention makes it \\nmore powerful than a traditional network. Several variants \\nof CNN are exist in the area that includes visual geometry \\ngroup (VGG) [38], AlexNet [62], Xception [17], Inception \\n[116], ResNet [39], etc. that can be used in various applica-\\ntion domains according to their learning capabilities.\\nRecurrent Neural Network (RNN) and\\xa0its Variants\\nA Recurrent Neural Network (RNN) is another popular neu-\\nral network, which employs sequential or time-series data \\nand feeds the output from the previous step as input to the \\ncurrent stage [27, 74]. Like feedforward and CNN, recurrent \\nnetworks learn from training input, however, distinguish by their “memory”, which allows them to impact current input \\nand output through using information from previous inputs. \\nUnlike typical DNN, which assumes that inputs and outputs \\nare independent of one another, the output of RNN is reliant \\non prior elements within the sequence. However, standard \\nrecurrent networks have the issue of vanishing gradients, \\nwhich makes learning long data sequences challenging. In \\nthe following, we discuss several popular variants of the \\nrecurrent network that minimizes the issues and perform \\nwell in many real-world application domains.\\n– Long short-term memory (LSTM) This is a popular form \\nof RNN architecture that uses special units to deal with \\nthe vanishing gradient problem, which was introduced by \\nHochreiter et\\xa0al. [42]. A memory cell in an LSTM unit \\ncan store data for long periods and the flow of informa-\\ntion into and out of the cell is managed by three gates. \\nFor instance, the ‘Forget Gate’ determines what informa-\\ntion from the previous state cell will be memorized and \\nwhat information will be removed that is no longer use-\\nful, while the ‘Input Gate’ determines which information \\nshould enter the cell state and the ‘Output Gate’ deter -\\nmines and controls the outputs. As it solves the issues \\nof training a recurrent network, the LSTM network is \\nconsidered one of the most successful RNN.\\n– Bidirectional RNN/LSTM Bidirectional RNNs connect \\ntwo hidden layers that run in opposite directions to a \\nsingle output, allowing them to accept data from both \\nthe past and future. Bidirectional RNNs, unlike tradi-\\ntional recurrent networks, are trained to predict both \\npositive and negative time directions at the same time. \\nA Bidirectional LSTM, often known as a BiLSTM, is \\nan extension of the standard LSTM that can increase \\nmodel performance on sequence classification issues \\n[113]. It is a sequence processing model comprising of \\ntwo LSTMs: one takes the input forward and the other \\ntakes it backward. Bidirectional LSTM in particular is a \\npopular choice in natural language processing tasks.\\n– Gated recurrent units (GRUs) A Gated Recurrent Unit \\n(GRU) is another popular variant of the recurrent net-\\nwork that uses gating methods to control and manage \\ninformation flow between cells in the neural network, \\nintroduced by Cho et\\xa0al. [16]. The GRU is like an LSTM, \\nhowever, has fewer parameters, as it has a reset gate and \\nFig. 7  An example of a convo-\\nlutional neural network (CNN \\nor ConvNet) including multiple \\nconvolution and pooling layers\\n\\n SN Computer Science (2021) 2:420\\n 420 Page 8 of 20\\nSN Computer Science\\nan update gate but lacks the output gate, as shown in \\nFig.\\xa0 8. Thus, the key difference between a GRU and an \\nLSTM is that a GRU has two gates (reset and update \\ngates) whereas an LSTM has three gates (namely input, \\noutput and forget gates). The GRU’s structure enables \\nit to capture dependencies from large sequences of data \\nin an adaptive manner, without discarding information \\nfrom earlier parts of the sequence. Thus GRU is a slightly \\nmore streamlined variant that often offers comparable \\nperformance and is significantly faster to compute [18]. \\nAlthough GRUs have been shown to exhibit better per -\\nformance on certain smaller and less frequent datasets \\n[18, 34], both variants of RNN have proven their effec-\\ntiveness while producing the outcome.\\nOverall, the basic property of a recurrent network is that \\nit has at least one feedback connection, which enables acti-\\nvations to loop. This allows the networks to do temporal \\nprocessing and sequence learning, such as sequence recogni-\\ntion or reproduction, temporal association or prediction, etc. \\nFollowing are some popular application areas of recurrent \\nnetworks such as prediction problems, machine translation, \\nnatural language processing, text summarization, speech \\nrecognition, and many more.\\nDeep Networks for\\xa0Generative or\\xa0Unsupervised \\nLearning\\nThis category of DL techniques is typically used to charac-\\nterize the high-order correlation properties or features for \\npattern analysis or synthesis, as well as the joint statistical \\ndistributions of the visible data and their associated classes \\n[21]. The key idea of generative deep architectures is that \\nduring the learning process, precise supervisory information such as target class labels is not of concern. As a result, \\nthe methods under this category are essentially applied for \\nunsupervised learning as the methods are typically used for \\nfeature learning or data generating and representation [ 20, \\n21]. Thus generative modeling can be used as preprocessing \\nfor the supervised learning tasks as well, which ensures the \\ndiscriminative model accuracy. Commonly used deep neural \\nnetwork techniques for unsupervised or generative learning \\nare Generative Adversarial Network (GAN), Autoencoder \\n(AE), Restricted Boltzmann Machine (RBM), Self-Organ-\\nizing Map (SOM), and Deep Belief Network (DBN) along \\nwith their variants.\\nGenerative Adversarial Network (GAN)\\nA Generative Adversarial Network (GAN), designed by Ian \\nGoodfellow [32], is a type of neural network architecture \\nfor generative modeling to create new plausible samples on \\ndemand. It involves automatically discovering and learning \\nregularities or patterns in input data so that the model may \\nbe used to generate or output new examples from the origi-\\nnal dataset. As shown in Fig.\\xa0 9, GANs are composed of two \\nneural networks, a generator G  that creates new data having \\nproperties similar to the original data, and a discriminator \\nD that predicts the likelihood of a subsequent sample being \\ndrawn from actual data rather than data provided by the \\ngenerator. Thus in GAN modeling, both the generator and \\ndiscriminator are trained to compete with each other. While \\nthe generator tries to fool and confuse the discriminator by \\ncreating more realistic data, the discriminator tries to distin-\\nguish the genuine data from the fake data generated by G.\\nGenerally, GAN network deployment is designed for \\nunsupervised learning tasks, but it has also proven to be a \\nbetter solution for semi-supervised and reinforcement learn-\\ning as well depending on the task [3 ]. GANs are also used \\nin state-of-the-art transfer learning research to enforce the \\nFig. 8  Basic structure of a gated recurrent unit (GRU) cell consisting \\nof reset and update gates\\nFig. 9  Schematic structure of a standard generative adversarial net-\\nwork (GAN)\\nSN Computer Science (2021) 2:420 \\n Page 9 of 20 420\\nSN Computer Science\\nalignment of the latent feature space [66]. Inverse models, \\nsuch as Bidirectional GAN (BiGAN) [25] can also learn a \\nmapping from data to the latent space, similar to how the \\nstandard GAN model learns a mapping from a latent space \\nto the data distribution. The potential application areas of \\nGAN networks are healthcare, image analysis, data augmen-\\ntation, video generation, voice generation, pandemics, traffic \\ncontrol, cybersecurity, and many more, which are increas-\\ning rapidly. Overall, GANs have established themselves as \\na comprehensive domain of independent data expansion and \\nas a solution to problems requiring a generative solution.\\nAuto‑Encoder (AE) and\\xa0Its Variants\\nAn auto-encoder (AE) [31] is a popular unsupervised learn -\\ning technique in which neural networks are used to learn \\nrepresentations. Typically, auto-encoders are used to work \\nwith high-dimensional data, and dimensionality reduction \\nexplains how a set of data is represented. Encoder, code, and \\ndecoder are the three parts of an autoencoder. The encoder \\ncompresses the input and generates the code, which the \\ndecoder subsequently uses to reconstruct the input. The \\nAEs have recently been used to learn generative data mod-\\nels [69]. The auto-encoder is widely used in many unsuper -\\nvised learning tasks, e.g., dimensionality reduction, feature \\nextraction, efficient coding, generative modeling, denoising, \\nanomaly or outlier detection, etc. [31, 132]. Principal com-\\nponent analysis (PCA) [99], which is also used to reduce the \\ndimensionality of huge data sets, is essentially similar to a \\nsingle-layered AE with a linear activation function. Regular -\\nized autoencoders such as sparse, denoising, and contractive \\nare useful for learning representations for later classification \\ntasks [119], while variational autoencoders can be used as \\ngenerative models [56], discussed below.\\n– Sparse Autoencoder (SAE) A sparse autoencoder [73] \\nhas a sparsity penalty on the coding layer as a part of its \\ntraining requirement. SAEs may have more hidden units \\nthan inputs, but only a small number of hidden units are \\npermitted to be active at the same time, resulting in a \\nsparse model. Figure\\xa0 10 shows a schematic structure of \\na sparse autoencoder with several active units in the hid-\\nden layer. This model is thus obliged to respond to the \\nunique statistical features of the training data following \\nits constraints.\\n– Denoising Autoencoder (DAE) A denoising autoencoder \\nis a variant on the basic autoencoder that attempts to \\nimprove representation (to extract useful features) by \\naltering the reconstruction criterion, and thus reduces the \\nrisk of learning the identity function [31, 119]. In other \\nwords, it receives a corrupted data point as input and is \\ntrained to recover the original undistorted input as its out-\\nput through minimizing the average reconstruction error over the training data, i.e, cleaning the corrupted input, or \\ndenoising. Thus, in the context of computing, DAEs can \\nbe considered as very powerful filters that can be utilized \\nfor automatic pre-processing. A denoising autoencoder, \\nfor example, could be used to automatically pre-process \\nan image, thereby boosting its quality for recognition \\naccuracy.\\n– Contractive Autoencoder (CAE) The idea behind a con-\\ntractive autoencoder, proposed by Rifai et\\xa0al. [90], is to \\nmake the autoencoders robust of small changes in the \\ntraining dataset. In its objective function, a CAE includes \\nan explicit regularizer that forces the model to learn an \\nencoding that is robust to small changes in input values. \\nAs a result, the learned representation’s sensitivity to the \\ntraining input is reduced. While DAEs encourage the \\nrobustness of reconstruction as discussed above, CAEs \\nencourage the robustness of representation.\\n– Variational Autoencoder (VAE) A variational autoen-\\ncoder [55] has a fundamentally unique property that \\ndistinguishes it from the classical autoencoder dis-\\ncussed above, which makes this so effective for gen -\\nerative modeling. VAEs, unlike the traditional autoen-\\nFig. 10  Schematic structure of a sparse autoencoder (SAE) with sev -\\neral active units (filled circle) in the hidden layer\\n SN Computer Science (2021) 2:420\\n 420 Page 10 of 20\\nSN Computer Science\\ncoders which map the input onto a latent vector, map \\nthe input data into the parameters of a probability dis-\\ntribution, such as the mean and variance of a Gaussian \\ndistribution. A VAE assumes that the source data has \\nan underlying probability distribution and then tries to \\ndiscover the distribution’s parameters. Although this \\napproach was initially designed for unsupervised learn-\\ning, its use has been demonstrated in other domains \\nsuch as semi-supervised learning [128] and supervised \\nlearning [51].\\nAlthough, the earlier concept of AE was typically for \\ndimensionality reduction or feature learning mentioned \\nabove, recently, AEs have been brought to the forefront of \\ngenerative modeling, even the generative adversarial net-\\nwork is one of the popular methods in the area. The AEs \\nhave been effectively employed in a variety of domains, \\nincluding healthcare, computer vision, speech recogni-\\ntion, cybersecurity, natural language processing, and many \\nmore. Overall, we can conclude that auto-encoder and its \\nvariants can play a significant role as unsupervised feature \\nlearning with neural network architecture.\\nKohonen Map or\\xa0Self‑Organizing Map (SOM)\\nA Self-Organizing Map (SOM) or Kohonen Map [59] is \\nanother form of unsupervised learning technique for creat-\\ning a low-dimensional (usually two-dimensional) represen-\\ntation of a higher-dimensional data set while maintaining \\nthe topological structure of the data. SOM is also known \\nas a neural network-based dimensionality reduction algo-\\nrithm that is commonly used for clustering [118]. A SOM \\nadapts to the topological form of a dataset by repeatedly \\nmoving its neurons closer to the data points, allowing us \\nto visualize enormous datasets and find probable clusters. \\nThe first layer of a SOM is the input layer, and the second \\nlayer is the output layer or feature map. Unlike other neu-\\nral networks that use error-correction learning, such as \\nbackpropagation with gradient descent [36], SOMs employ \\ncompetitive learning, which uses a neighborhood function \\nto retain the input space’s topological features. SOM is \\nwidely utilized in a variety of applications, including pat-\\ntern identification, health or medical diagnosis, anomaly \\ndetection, and virus or worm attack detection [60, 87]. The \\nprimary benefit of employing a SOM is that this can make \\nhigh-dimensional data easier to visualize and analyze to \\nunderstand the patterns. The reduction of dimensionality \\nand grid clustering makes it easy to observe similarities \\nin the data. As a result, SOMs can play a vital role in \\ndeveloping a data-driven effective model for a particular \\nproblem domain, depending on the data characteristics.Restricted Boltzmann Machine (RBM)\\nA Restricted Boltzmann Machine (RBM) [75] is also a gen-\\nerative stochastic neural network capable of learning a prob-\\nability distribution across its inputs. Boltzmann machines \\ntypically consist of visible and hidden nodes and each node \\nis connected to every other node, which helps us understand \\nirregularities by learning how the system works in normal \\ncircumstances. RBMs are a subset of Boltzmann machines \\nthat have a limit on the number of connections between the \\nvisible and hidden layers [77]. This restriction permits train-\\ning algorithms like the gradient-based contrastive divergence \\nalgorithm to be more efficient than those for Boltzmann \\nmachines in general [ 41]. RBMs have found applications \\nin dimensionality reduction, classification, regression, col-\\nlaborative filtering, feature learning, topic modeling, and \\nmany others. In the area of deep learning modeling, they \\ncan be trained either supervised or unsupervised, depend-\\ning on the task. Overall, the RBMs can recognize patterns \\nin data automatically and develop probabilistic or stochastic \\nmodels, which are utilized for feature selection or extraction, \\nas well as forming a deep belief network.\\nDeep Belief Network (DBN)\\nA Deep Belief Network (DBN) [40] is a multi-layer genera-\\ntive graphical model of stacking several individual unsu -\\npervised networks such as AEs or RBMs, that use each \\nnetwork’s hidden layer as the input for the next layer, i.e, \\nconnected sequentially. Thus, we can divide a DBN into \\n(i) AE-DBN which is known as stacked AE, and (ii) RBM-\\nDBN that is known as stacked RBM, where AE-DBN is \\ncomposed of autoencoders and RBM-DBN is composed \\nof restricted Boltzmann machines, discussed earlier. The \\nultimate goal is to develop a faster-unsupervised training \\ntechnique for each sub-network that depends on contrastive \\ndivergence [41]. DBN can capture a hierarchical representa-\\ntion of input data based on its deep structure. The primary \\nidea behind DBN is to train unsupervised feed-forward \\nneural networks with unlabeled data before fine-tuning \\nthe network with labeled input. One of the most important \\nadvantages of DBN, as opposed to typical shallow learning \\nnetworks, is that it permits the detection of deep patterns, \\nwhich allows for reasoning abilities and the capture of the \\ndeep difference between normal and erroneous data [89]. A \\ncontinuous DBN is simply an extension of a standard DBN \\nthat allows a continuous range of decimals instead of binary \\ndata. Overall, the DBN model can play a key role in a wide \\nrange of high-dimensional data applications due to its strong \\nfeature extraction and classification capabilities and become \\none of the significant topics in the field of neural networks.\\nIn summary, the generative learning techniques discussed \\nabove typically allow us to generate a new representation \\nSN Computer Science (2021) 2:420 \\n Page 11 of 20 420\\nSN Computer Science\\nof data through exploratory analysis. As a result, these \\ndeep generative networks can be utilized as preprocessing \\nfor supervised or discriminative learning tasks, as well as \\nensuring model accuracy, where unsupervised representation \\nlearning can allow for improved classifier generalization.\\nDeep Networks for\\xa0Hybrid Learning and\\xa0Other \\nApproaches\\nIn addition to the above-discussed deep learning categories, \\nhybrid deep networks and several other approaches such as \\ndeep transfer learning (DTL) and deep reinforcement learn-\\ning (DRL) are popular, which are discussed in the following.\\nHybrid Deep Neural Networks\\nGenerative models are adaptable, with the capacity to learn \\nfrom both labeled and unlabeled data. Discriminative mod-\\nels, on the other hand, are unable to learn from unlabeled \\ndata yet outperform their generative counterparts in super -\\nvised tasks. A framework for training both deep generative \\nand discriminative models simultaneously can enjoy the \\nbenefits of both models, which motivates hybrid networks.\\nHybrid deep learning models are typically composed of \\nmultiple (two or more) deep basic learning models, where \\nthe basic model is a discriminative or generative deep learn-\\ning model discussed earlier. Based on the integration of dif-\\nferent basic generative or discriminative models, the below \\nthree categories of hybrid deep learning models might be \\nuseful for solving real-world problems. These are as follows:\\n– Hybrid Model _1 : An integration of different generative \\nor discriminative models to extract more meaningful \\nand robust features. Examples could be CNN+LSTM, \\nAE+GAN, and so on.\\n– Hybrid Model _2 : An integration of generative model \\nfollowed by a discriminative model. Examples could be \\nDBN+MLP, GAN+CNN, AE+CNN, and so on.\\n– Hybrid Model _3 : An integration of generative or discrim-\\ninative model followed by a non-deep learning classifier. \\nExamples could be AE+SVM, CNN+SVM, and so on.\\nThus, in a broad sense, we can conclude that hybrid mod-\\nels can be either classification-focused or non-classification \\ndepending on the target use. However, most of the hybrid \\nlearning-related studies in the area of deep learning are \\nclassification-focused or supervised learning tasks, sum-\\nmarized in Table\\xa0 1. The unsupervised generative models \\nwith meaningful representations are employed to enhance \\nthe discriminative models. The generative models with use-\\nful representation can provide more informative and low-\\ndimensional features for discrimination, and they can also enable to enhance the training data quality and quantity, \\nproviding additional information for classification.\\nDeep Transfer Learning (DTL)\\nTransfer Learning is a technique for effectively using previ-\\nously learned model knowledge to solve a new task with \\nminimum training or fine-tuning. In comparison to typical \\nmachine learning techniques [97], DL takes a large amount \\nof training data. As a result, the need for a substantial vol -\\nume of labeled data is a significant barrier to address some \\nessential domain-specific tasks, particularly, in the medical \\nsector, where creating large-scale, high-quality annotated \\nmedical or health datasets is both difficult and costly. Fur -\\nthermore, the standard DL model demands a lot of computa-\\ntional resources, such as a GPU-enabled server, even though \\nresearchers are working hard to improve it. As a result, Deep \\nTransfer Learning (DTL), a DL-based transfer learning \\nmethod, might be helpful to address this issue. Figure\\xa0 11 \\nshows a general structure of the transfer learning process, \\nwhere knowledge from the pre-trained model is transferred \\ninto a new DL model. It’s especially popular in deep learning \\nright now since it allows to train deep neural networks with \\nvery little data [126].\\nTransfer learning is a two-stage approach for training a \\nDL model that consists of a pre-training step and a fine-\\ntuning step in which the model is trained on the target task. \\nSince deep neural networks have gained popularity in a vari-\\nety of fields, a large number of DTL methods have been pre-\\nsented, making it crucial to categorize and summarize them. \\nBased on the techniques used in the literature, DTL can be \\nclassified into four categories [117]. These are (i) instances-\\nbased deep transfer learning that utilizes instances in source \\ndomain by appropriate weight, (ii) mapping-based deep \\ntransfer learning that maps instances from two domains into \\na new data space with better similarity, (iii) network-based \\ndeep transfer learning that reuses the partial of network pre-\\ntrained in the source domain, and (iv) adversarial based deep \\ntransfer learning that uses adversarial technology to find \\ntransferable features that both suitable for two domains. Due \\nto its high effectiveness and practicality, adversarial-based \\ndeep transfer learning has exploded in popularity in recent \\nyears. Transfer learning can also be classified into inductive, \\ntransductive, and unsupervised transfer learning depending \\non the circumstances between the source and target domains \\nand activities [81]. While most current research focuses on \\nsupervised learning, how deep neural networks can transfer \\nknowledge in unsupervised or semi-supervised learning may \\ngain further interest in the future. DTL techniques are useful \\nin a variety of fields including natural language processing, \\nsentiment classification, visual recognition, speech recogni-\\ntion, spam filtering, and relevant others.\\n SN Computer Science (2021) 2:420\\n 420 Page 12 of 20\\nSN Computer Science\\nTable 1  A summary of deep learning tasks and methods in several popular real-world applications areas\\nApplication areas Tasks Methods References\\nHealthcare and Medical applications Regular health factors analysis CNN-based Ismail et\\xa0al. [48]\\nIdentifying malicious behaviors RNN-based Xue et\\xa0al. [129]\\nCoronary heart disease risk prediction Autoencoder based Amarbayasgalan et\\xa0al. [6]\\nCancer classification Transfer learning based Sevakula et\\xa0al. [110]\\nDiagnosis of COVID-19 CNN and BiLSTM based Aslan et\\xa0al. [10]\\nDetection of COVID-19 CNN-LSTM based Islam et\\xa0al. [47]\\nNatural Language Processing Text summarization Auto-encoder based Yousefi et\\xa0al. [130]\\nSentiment analysis CNN-LSTM based Wang et\\xa0al. [120]\\nSentiment analysis CNN and Bi-LSTM based Minaee et\\xa0al. [78]\\nAspect-level sentiment classification Attention-based LSTM Wang et\\xa0al. [124]\\nSpeech recognition Distant speech recognition Attention-based LSTM Zhang et\\xa0al. [135]\\nSpeech emotion classification Transfer learning based Latif et\\xa0al. [63]\\nEmotion recognition from speech CNN and LSTM based Satt et\\xa0al. [109]\\nCybersecurity Zero-day malware detection Autoencoders and GAN based Kim et\\xa0al. [54]\\nSecurity incidents and fraud analysis SOM-based Lopez et\\xa0al. [70]\\nAndroid malware detection Autoencoder and CNN based Wang et\\xa0al. [122]\\nintrusion detection classification DBN-based Wei et\\xa0al. [125]\\nDoS attack detection RBM-based Imamverdiyev et\\xa0al. [46]\\nSuspicious flow detection Hybrid deep-learning-based Garg et\\xa0al. [29]\\nNetwork intrusion detection AE and SVM based Al et\\xa0al. [4]\\nIoT and Smart cities Smart energy management CNN and Attention mechanism Abdel et\\xa0al. [2]\\nParticulate matter forecasting CNN-LSTM based Huang et\\xa0al. [43]\\nSmart parking system CNN-LSTM based Piccialli et\\xa0al. [85]\\nDisaster management DNN-based Aqib et\\xa0al. [8]\\nAir quality prediction LSTM-RNN based Kok et\\xa0al. [61]\\nCybersecurity in smart cities RBM, DBN, RNN, CNN, GAN Chen et\\xa0al. [15]\\nSmart Agriculture A smart agriculture IoT system RL-based Bu et\\xa0al. [11]\\nPlant disease detection CNN-based Ale et\\xa0al. [5]\\nAutomated soil quality evaluation DNN-based Sumathi et\\xa0al. [115]\\nBusiness and Financial Services Predicting customers’ purchase behavior DNN based Chaudhuri [14]\\nStock trend prediction CNN and LSTM based anuradha et\\xa0al. [7]\\nFinancial loan default prediction CNN-based Deng et\\xa0al. [23]\\nPower consumption forecasting LSTM-based Shao et\\xa0al. [112]\\nVirtual Assistant and Chatbot Services An intelligent chatbot Bi-RNN and Attention model Dhyani et\\xa0al. [24]\\nVirtual listener agent GRU and LSTM based Huang et\\xa0al. [44]\\nSmart blind assistant CNN-based Rahman et\\xa0al. [88]\\nObject Detection and Recognition Object detection in X-ray images CNN-based Gu et\\xa0al. [35]\\nObject detection for disaster response CNN-based Pi et\\xa0al. [84]\\nMedicine recognition system CNN-based Chang et\\xa0al. [12]\\nFace recognition in IoT-cloud environ-\\nmentCNN-based Masud et\\xa0al. [76]\\nFood recognition system CNN-based Liu et\\xa0al. [68]\\nAffect recognition system DBN-based Kawde et\\xa0al. [53]\\nFacial expression analysis CNN and LSTM based Li et\\xa0al. [67]\\nRecommendation and Intelligent system Hybrid recommender system DNN-based Kiran et\\xa0al. [57]\\nVisual recommendation and search CNN-based Shankar et\\xa0al. [111]\\nRecommendation system CNN and Bi-LSTM based Rosa et\\xa0al. [91]\\nIntelligent system for impaired patients RL-based Naeem et\\xa0al. [79]\\nIntelligent transportation system CNN-based Wang et\\xa0al. [123]\\nSN Computer Science (2021) 2:420 \\n Page 13 of 20 420\\nSN Computer Science\\nDeep Reinforcement Learning (DRL)\\nReinforcement learning takes a different approach to solv -\\ning the sequential decision-making problem than other \\napproaches we have discussed so far. The concepts of an \\nenvironment and an agent are often introduced first in \\nreinforcement learning. The agent can perform a series of \\nactions in the environment, each of which has an impact on \\nthe environment’s state and can result in possible rewards \\n(feedback) - “positive” for good sequences of actions that \\nresult in a “good” state, and “negative” for bad sequences \\nof actions that result in a “bad” state. The purpose of rein-\\nforcement learning is to learn good action sequences through \\ninteraction with the environment, typically referred to as a \\npolicy.\\nDeep reinforcement learning (DRL or deep RL) [9 ] inte-\\ngrates neural networks with a reinforcement learning archi-\\ntecture to allow the agents to learn the appropriate actions \\nin a virtual environment, as shown in Fig.\\xa0 12. In the area \\nof reinforcement learning, model-based RL is based on \\nlearning a transition model that enables for modeling of the \\nenvironment without interacting with it directly, whereas \\nmodel-free RL methods learn directly from interactions with \\nthe environment. Q-learning is a popular model-free RL \\ntechnique for determining the best action-selection policy \\nfor any (finite) Markov Decision Process (MDP) [86, 97]. \\nMDP is a mathematical framework for modeling decisions \\nbased on state, action, and rewards [86]. In addition, Deep \\nQ-Networks, Double DQN, Bi-directional Learning, Monte \\nCarlo Control, etc. are used in the area [ 50, 97]. In DRL \\nmethods it incorporates DL models, e.g. Deep Neural Net-\\nworks (DNN), based on MDP principle [71], as policy and/\\nor value function approximators. CNN for example can be \\nused as a component of RL agents to learn directly from raw, high-dimensional visual inputs. In the real world, DRL-\\nbased solutions can be used in several application areas \\nincluding robotics, video games, natural language process-\\ning, computer vision, and relevant others.\\nDeep Learning Application Summary\\nDuring the past few years, deep learning has been success-\\nfully applied to numerous problems in many application \\nareas. These include natural language processing, senti-\\nment analysis, cybersecurity, business, virtual assistants, \\nvisual recognition, healthcare, robotics, and many more. In \\nFig.\\xa0 13, we have summarized several potential real-world \\napplication areas of deep learning. Various deep learning \\ntechniques according to our presented taxonomy in Fig.\\xa0 6 \\nthat includes discriminative learning, generative learning, \\nas well as hybrid models, discussed earlier, are employed in \\nthese application areas. In Table\\xa0 1, we have also summarized Fig. 11  A general structure of \\ntransfer learning process, where \\nknowledge from pre-trained \\nmodel is transferred into new \\nDL model\\nFig. 12  Schematic structure of deep reinforcement learning (DRL) \\nhighlighting a deep neural network\\n SN Computer Science (2021) 2:420\\n 420 Page 14 of 20\\nSN Computer Science\\nvarious deep learning tasks and techniques that are used to \\nsolve the relevant tasks in several real-world applications \\nareas. Overall, from Fig.\\xa0 13 and Table\\xa0 1, we can conclude \\nthat the future prospects of deep learning modeling in real-\\nworld application areas are huge and there are lots of scopes \\nto work. In the next section, we also summarize the research \\nissues in deep learning modeling and point out the potential \\naspects for future generation DL modeling.\\nResearch Directions and\\xa0Future Aspects\\nWhile existing methods have established a solid foundation \\nfor deep learning systems and research, this section outlines \\nthe below ten potential future research directions based on \\nour study.\\n– Automation in Data Annotation According to the existing \\nliterature, discussed in Section\\xa0 3, most of the deep learn-\\ning models are trained through publicly available datasets \\nthat are annotated. However, to build a system for a new \\nproblem domain or recent data-driven system, raw data \\nfrom relevant sources are needed to collect. Thus, data annotation, e.g., categorization, tagging, or labeling of a \\nlarge amount of raw data, is important for building dis-\\ncriminative deep learning models or supervised tasks, \\nwhich is challenging. A technique with the capability of \\nautomatic and dynamic data annotation, rather than man-\\nual annotation or hiring annotators, particularly, for large \\ndatasets,\\xa0could be more effective for supervised learning \\nas well as minimizing human effort. Therefore, a more \\nin-depth investigation of data collection and annotation \\nmethods, or designing an unsupervised learning-based \\nsolution could be one of the primary research directions \\nin the area of deep learning modeling.\\n– Data Preparation for Ensuring Data Quality As dis-\\ncussed earlier throughout the paper, the deep learning \\nalgorithms highly impact data quality, and availability \\nfor training, and consequently on the resultant model for \\na particular problem domain. Thus, deep learning models \\nmay become worthless or yield decreased accuracy if \\nthe data is bad, such as data sparsity, non-representative, \\npoor-quality, ambiguous values, noise, data imbalance, \\nirrelevant features, data inconsistency, insufficient quan-\\ntity, and so on for training. Consequently, such issues \\nin data can lead to poor processing and inaccurate find-\\nFig. 13  Several potential real-world application areas of deep learning\\nSN Computer Science (2021) 2:420 \\n Page 15 of 20 420\\nSN Computer Science\\nings, which is a major problem while discovering insights \\nfrom data. Thus deep learning models also need to adapt \\nto such rising issues in data, to capture approximated \\ninformation from observations. Therefore, effective data \\npre-processing techniques are needed to design accord-\\ning to the nature of the data problem and characteristics, \\nto handling such emerging challenges, which could be \\nanother research direction in the area.\\n– Black-box Perception and Proper DL/ML Algorithm \\nSelection  In general, it’s difficult to explain how a deep \\nlearning result is obtained or how they get the ultimate \\ndecisions for a particular model. Although DL models \\nachieve significant performance while learning from \\nlarge datasets, as discussed in Section\\xa0 2, this “black-box” \\nperception of DL modeling typically represents weak \\nstatistical interpretability that could be a major issue in \\nthe area. On the other hand, ML algorithms, particularly, \\nrule-based machine learning techniques provide explicit \\nlogic rules (IF-THEN) for making decisions that are eas-\\nier to interpret, update or delete according to the target \\napplications [97, 100, 105]. If the wrong learning algo-\\nrithm is chosen, unanticipated results may occur, result-\\ning in a loss of effort as well as the model’s efficacy and \\naccuracy. Thus by taking into account the performance, \\ncomplexity, model accuracy, and applicability, selecting \\nan appropriate model for the target application is chal -\\nlenging, and in-depth analysis is needed for better under -\\nstanding and decision making.\\n– Deep Networks for Supervised or Discriminative Learn-\\ning: According to our designed taxonomy of deep learn-\\ning techniques, as shown in Fig.\\xa0 6, discriminative archi-\\ntectures mainly include MLP, CNN, and RNN, along \\nwith their variants that are applied widely in various \\napplication domains. However, designing new techniques \\nor their variants of such discriminative techniques by tak -\\ning into account model optimization, accuracy, and appli-\\ncability, according to the target real-world application \\nand the nature of the data, could be a novel contribution, \\nwhich can also be considered as a major future aspect in \\nthe area of supervised or discriminative learning.\\n– Deep Networks for Unsupervised or Generative Learn-\\ning As discussed in Section\\xa0 3, unsupervised learning or \\ngenerative deep learning modeling is one of the major \\ntasks in the area, as it allows us to characterize the \\nhigh-order correlation properties or features in data, or \\ngenerating a new representation of data through explor -\\natory analysis. Moreover, unlike supervised learning \\n[97], it does not require labeled data due to its capa-\\nbility to derive insights directly from the data as well \\nas data-driven decision making. Consequently, it thus \\ncan be used as preprocessing for supervised learning \\nor discriminative modeling as well as semi-supervised \\nlearning tasks, which ensure learning accuracy and model efficiency. According to our designed taxonomy \\nof deep learning techniques, as shown in Fig.\\xa0 6, genera-\\ntive techniques mainly include GAN, AE, SOM, RBM, \\nDBN, and their variants. Thus, designing new tech-\\nniques or their variants for an effective data modeling \\nor representation according to the target real-world \\napplication could be a novel contribution, which can \\nalso be considered as a major future aspect in the area \\nof unsupervised or generative learning.\\n– Hybrid/Ensemble Modeling and Uncertainty Handling  \\nAccording to our designed taxonomy of DL techniques, \\nas shown in Fig 6 , this is considered as another major \\ncategory in deep learning tasks. As hybrid modeling \\nenjoys the benefits of both generative and discrimina-\\ntive learning, an effective hybridization can outperform \\nothers in terms of performance as well as uncertainty \\nhandling in high-risk applications. In Section\\xa0 3, we \\nhave summarized various types of hybridization, e.g., \\nAE+CNN/SVM. Since a group of neural networks is \\ntrained with distinct parameters or with separate sub-\\nsampling training datasets, hybridization or ensem-\\nbles of such techniques, i.e., DL with DL/ML, can \\nplay a key role in the area. Thus designing effective \\nblended discriminative and generative models accord-\\ningly rather than naive method, could be an important \\nresearch opportunity to solve various real-world issues \\nincluding semi-supervised learning tasks and model \\nuncertainty.\\n– Dynamism in Selecting Threshold/ Hyper-parameters \\nValues, and Network Structures with Computational Effi-\\nciency In general, the relationship among performance, \\nmodel complexity, and computational requirements is a \\nkey issue in deep learning modeling and applications. A \\ncombination of algorithmic advancements with improved \\naccuracy as well as maintaining computational efficiency, \\ni.e., achieving the maximum throughput while consum-\\ning the least amount of resources, without significant \\ninformation loss, can lead to a breakthrough in the effec-\\ntiveness of deep learning modeling in future real-world \\napplications. The concept of incremental approaches or \\nrecency-based learning [100] might be effective in sev -\\neral cases depending on the nature of target applications. \\nMoreover, assuming the network structures with a static \\nnumber of nodes and layers, hyper-parameters values or \\nthreshold settings, or selecting them by the trial-and-\\nerror process may not be effective in many cases, as it \\ncan be changed due to the changes in data. Thus, a data-\\ndriven approach to select them dynamically could be \\nmore effective while building a deep learning model in \\nterms of both performance and real-world applicability. \\nSuch type of data-driven automation can lead to future \\ngeneration deep learning modeling with additional intel-\\nligence, which could be a significant future aspect in the \\n SN Computer Science (2021) 2:420\\n 420 Page 16 of 20\\nSN Computer Science\\narea as well as an important research direction to contrib-\\nute.\\n– Lightweight Deep Learning Modeling for Next-Gener -\\nation Smart Devices and Applications: In recent years, \\nthe Internet of Things (IoT) consisting of billions of \\nintelligent and communicating things and mobile com-\\nmunications technologies have become popular to detect \\nand gather human and environmental information (e.g. \\ngeo-information, weather data, bio-data, human behav -\\niors, and so on) for a variety of intelligent services and \\napplications. Every day, these ubiquitous smart things or \\ndevices generate large amounts of data, requiring rapid \\ndata processing on a variety of smart mobile devices \\n[72]. Deep learning technologies can be incorporate to \\ndiscover underlying properties and to effectively han-\\ndle such large amounts of sensor data for a variety of \\nIoT applications including health monitoring and dis-\\nease analysis, smart cities, traffic flow prediction, and \\nmonitoring, smart transportation, manufacture inspec-\\ntion, fault assessment, smart industry or Industry 4.0, \\nand many more. Although deep learning techniques \\ndiscussed in Section\\xa0 3 are considered as powerful tools \\nfor processing big data, lightweight modeling is impor -\\ntant for resource-constrained devices, due to their high \\ncomputational cost and considerable memory overhead. \\nThus several techniques such as optimization, simplifi-\\ncation, compression, pruning, generalization, important \\nfeature extraction, etc. might be helpful in several cases. \\nTherefore, constructing the lightweight deep learning \\ntechniques based on a baseline network architecture to \\nadapt the DL model for next-generation mobile, IoT, or \\nresource-constrained devices and applications, could be \\nconsidered as a significant future aspect in the area.\\n– Incorporating Domain Knowledge into Deep Learn-\\ning Modeling Domain knowledge, as opposed to general \\nknowledge or domain-independent knowledge, is knowl-\\nedge of a specific, specialized topic or field. For instance, \\nin terms of natural language processing, the properties \\nof the English language typically differ from other lan -\\nguages like Bengali, Arabic, French, etc. Thus integrating \\ndomain-based constraints into the deep learning model \\ncould produce better results for such particular purpose. \\nFor instance, a task-specific feature extractor considering \\ndomain knowledge in smart manufacturing for fault diag-\\nnosis can resolve the issues in traditional deep-learning-\\nbased methods [28]. Similarly, domain knowledge in medi-\\ncal image analysis [58], financial sentiment analysis [49], \\ncybersecurity analytics [94, 103] as well as conceptual data \\nmodel in which semantic information, (i.e., meaningful for \\na system, rather than merely correlational) [ 45, 121, 131] is \\nincluded, can play a vital role in the area. Transfer learning \\ncould be an effective way to get started on a new challenge \\nwith domain knowledge. Moreover, contextual information such as spatial, temporal, social, environmental contexts \\n[92, 104, 108] can also play an important role to incorpo-\\nrate context-aware computing with domain knowledge for \\nsmart decision making as well as building adaptive and \\nintelligent context-aware systems. Therefore understanding \\ndomain knowledge and effectively incorporating them into \\nthe deep learning model could be another research direc-\\ntion.\\n– Designing General Deep Learning Framework for Target \\nApplication Domains One promising research direction \\nfor deep learning-based solutions is to develop a general \\nframework that can handle data diversity, dimensions, stim-\\nulation types, etc. The general framework would require \\ntwo key capabilities: the attention mechanism that focuses \\non the most valuable parts of input signals, and the abil-\\nity to capture latent feature that enables the framework to \\ncapture the distinctive and informative features. Attention \\nmodels have been a popular research topic because of their \\nintuition, versatility, and interpretability, and employed in \\nvarious application areas like computer vision, natural lan-\\nguage processing, text or image classification, sentiment \\nanalysis, recommender systems, user profiling, etc [ 13, \\n80]. Attention mechanism can be implemented based on \\nlearning algorithms such as reinforcement learning that is \\ncapable of finding the most useful part through a policy \\nsearch [133, 134]. Similarly, CNN can be integrated with \\nsuitable attention mechanisms to form a general classifi-\\ncation framework, where CNN can be used as a feature \\nlearning tool for capturing features in various levels and \\nranges. Thus, designing a general deep learning framework \\nconsidering attention as well as a latent feature for target \\napplication domains could be another area to contribute.\\nTo summarize, deep learning is a fairly open topic to which \\nacademics can contribute by developing new methods or \\nimproving existing methods to handle the above-mentioned \\nconcerns and tackle real-world problems in a variety of \\napplication areas. This can also help the researchers con-\\nduct a thorough analysis of the application’s hidden and \\nunexpected challenges to produce more reliable and realis-\\ntic outcomes. Overall, we can conclude that addressing the \\nabove-mentioned issues and contributing to proposing effec-\\ntive and efficient techniques could lead to “Future Genera-\\ntion DL” modeling as well as more intelligent and automated \\napplications.\\nConcluding Remarks\\nIn this article, we have presented a structured and compre-\\nhensive view of deep learning technology, which is consid-\\nered a core part of artificial intelligence as well as data sci-\\nence. It starts with a history of artificial neural networks and \\nSN Computer Science (2021) 2:420 \\n Page 17 of 20 420\\nSN Computer Science\\nmoves to recent deep learning techniques and breakthroughs \\nin different applications. Then, the key algorithms in this \\narea, as well as deep neural network modeling in various \\ndimensions are explored. For this, we have also presented a \\ntaxonomy considering the variations of deep learning tasks \\nand how they are used for different purposes. In our compre-\\nhensive study, we have taken into account not only the deep \\nnetworks for supervised or discriminative learning but also \\nthe deep networks for unsupervised or generative learning, \\nand hybrid learning that can be used to solve a variety of \\nreal-world issues according to the nature of problems.\\nDeep learning, unlike traditional machine learning and \\ndata mining algorithms, can produce extremely high-level \\ndata representations from enormous amounts of raw data. As \\na result, it has provided an excellent solution to a variety of \\nreal-world problems. A successful deep learning technique \\nmust possess the relevant data-driven modeling depending \\non the characteristics of raw data. The sophisticated learn -\\ning algorithms then need to be trained through the collected \\ndata and knowledge related to the target application before \\nthe system can assist with intelligent decision-making. Deep \\nlearning has shown to be useful in a wide range of applica-\\ntions and research areas such as healthcare, sentiment analy -\\nsis, visual recognition, business intelligence, cybersecurity, \\nand many more that are summarized in the paper.\\nFinally, we have summarized and discussed the chal-\\nlenges faced and the potential research directions, and future \\naspects in the area. Although deep learning is considered \\na black-box solution for many applications due to its poor \\nreasoning and interpretability, addressing the challenges or \\nfuture aspects that are identified could lead to future genera-\\ntion deep learning modeling and smarter systems. This can \\nalso help the researchers for in-depth analysis to produce \\nmore reliable and realistic outcomes. Overall, we believe \\nthat our study on neural networks and deep learning-based \\nadvanced analytics points in a promising path and can be uti-\\nlized as a reference guide for future research and implemen-\\ntations in relevant application domains by both academic \\nand industry professionals.\\nDeclarations  \\nConflict of interest The author declares no conflict of interest.\\nReferences\\n 1. Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin \\nMa, Ghemawat S, Irving G, Isard M, et\\xa0al. Tensorflow: a system \\nfor large-scale machine learning. In: 12th {USENIX} Symposium \\non operating systems design and implementation ({OSDI} 16), \\n2016; p. 265–283. 2. Abdel-Basset M, Hawash H, Chakrabortty RK, Ryan M. \\nEnergy-net: a deep learning approach for smart energy man-\\nagement in iot-based smart cities. IEEE Internet of Things J. \\n2021.\\n 3. Aggarwal A, Mittal M, Battineni G. Generative adversarial net-\\nwork: an overview of theory and applications. Int J Inf Manag \\nData Insights. 2021; p. 100004.\\n 4. Al-Qatf M, Lasheng Y, Al-Habib M, Al-Sabahi K. Deep learning \\napproach combining sparse autoencoder with svm for network \\nintrusion detection. IEEE Access. 2018;6:52843–56.\\n 5. Ale L, Sheta A, Li L, Wang Y, Zhang N. Deep learning based \\nplant disease detection for smart agriculture. In: 2019 IEEE \\nGlobecom Workshops (GC Wkshps), 2019; p. 1–6. IEEE.\\n 6. Amarbayasgalan T, Lee JY, Kim KR, Ryu KH. Deep autoencoder \\nbased neural networks for coronary heart disease risk prediction. \\nIn: Heterogeneous data management, polystores, and analytics \\nfor healthcare. Springer; 2019. p. 237–48.\\n 7. Anuradha J, et\\xa0al. Big data based stock trend prediction using \\ndeep cnn with reinforcement-lstm model. Int J Syst Assur Eng \\nManag. 2021; p. 1–11.\\n 8. Aqib M, Mehmood R, Albeshri A, Alzahrani A. Disaster man -\\nagement in smart cities by forecasting traffic plan using deep \\nlearning and gpus. In: International Conference on smart cities, \\ninfrastructure, technologies and applications. Springer; 2017. p. \\n139–54.\\n 9. Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. Deep \\nreinforcement learning: a brief survey. IEEE Signal Process Mag. \\n2017;34(6):26–38.\\n 10. Aslan MF, Unlersen MF, Sabanci K, Durdu A. Cnn-based trans-\\nfer learning-bilstm network: a novel approach for covid-19 infec-\\ntion detection. Appl Soft Comput. 2021;98:106912.\\n 11. Bu F, Wang X. A smart agriculture iot system based on deep rein-\\nforcement learning. Futur Gener Comput Syst. 2019;99:500–7.\\n 12. Chang W-J, Chen L-B, Hsu C-H, Lin C-P, Yang T-C. A deep \\nlearning-based intelligent medicine recognition system for \\nchronic patients. IEEE Access. 2019;7:44441–58.\\n 13. Chaudhari S, Mithal V, Polatkan Gu, Ramanath R. An attentive \\nsurvey of attention models. arXiv preprint arXiv:1904.02874, \\n2019.\\n 14. Chaudhuri N, Gupta G, Vamsi V, Bose I. On the platform but will \\nthey buy? predicting customers’ purchase behavior using deep \\nlearning. Decis Support Syst. 2021; p. 113622.\\n 15. Chen D, Wawrzynski P, Lv Z. Cyber security in smart cities: \\na review of deep learning-based applications and case studies. \\nSustain Cities Soc. 2020; p. 102655.\\n 16. Cho K, Van\\xa0MB, Gulcehre C, Bahdanau D, Bougares F, Schwenk \\nH, Bengio Y. Learning phrase representations using rnn encoder-\\ndecoder for statistical machine translation. arXiv preprint \\narXiv:1406.1078, 2014.\\n 17. Chollet F. Xception: Deep learning with depthwise separable \\nconvolutions. In: Proceedings of the IEEE Conference on com-\\nputer vision and pattern recognition, 2017; p. 1251–258.\\n 18. Chung J, Gulcehre C, Cho KH, Bengio Y. Empirical evaluation \\nof gated recurrent neural networks on sequence modeling. arXiv \\npreprint arXiv:1412.3555, 2014.\\n 19. Coelho IM, Coelho VN, da Eduardo J, Luz S, Ochi LS, Guima-\\nrães FG, Rios E. A gpu deep learning metaheuristic based model \\nfor time series forecasting. Appl Energy. 2017;201:412–8.\\n 20. Da'u A, Salim N. Recommendation system based on deep learn-\\ning methods: a systematic review and new directions. Artif Intel \\nRev. 2020;53(4):2709–48.\\n 21. Deng L. A tutorial survey of architectures, algorithms, and appli -\\ncations for deep learning. APSIPA Trans Signal Inf Process. \\n2014; p. 3.\\n 22. Deng L, Dong Yu. Deep learning: methods and applications. \\nFound Trends Signal Process. 2014;7(3–4):197–387.\\n SN Computer Science (2021) 2:420\\n 420 Page 18 of 20\\nSN Computer Science\\n 23. Deng S, Li R, Jin Y, He H. Cnn-based feature cross and clas-\\nsifier for loan default prediction. In: 2020 International Con-\\nference on image, video processing and artificial intelligence, \\nvolume 11584, page 115841K. International Society for Optics \\nand Photonics, 2020.\\n 24. Dhyani M, Kumar R. An intelligent chatbot using deep learning \\nwith bidirectional rnn and attention model. Mater Today Proc. \\n2021;34:817–24.\\n 25. Donahue J, Krähenbühl P, Darrell T. Adversarial feature learn-\\ning. arXiv preprint arXiv:1605.09782, 2016.\\n 26. Du K-L, Swamy MNS. Neural networks and statistical learn-\\ning. Berlin: Springer Science & Business Media; 2013.\\n 27. Dupond S. A thorough review on the current advance of neural \\nnetwork structures. Annu Rev Control. 2019;14:200–30.\\n 28. Feng J, Yao Y, Lu S, Liu Y. Domain knowledge-based deep-\\nbroad learning framework for fault diagnosis. IEEE Trans Ind \\nElectron. 2020;68(4):3454–64.\\n 29. Garg S, Kaur K, Kumar N, Rodrigues JJPC. Hybrid deep-\\nlearning-based anomaly detection scheme for suspicious flow \\ndetection in sdn: a social multimedia perspective. IEEE Trans \\nMultimed. 2019;21(3):566–78.\\n 30. Géron A. Hands-on machine learning with Scikit-Learn, Keras. \\nIn: and TensorFlow: concepts, tools, and techniques to build \\nintelligent systems. O’Reilly Media; 2019.\\n 31. Goodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning, \\nvol. 1. Cambridge: MIT Press; 2016.\\n 32. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley \\nD, Ozair S, Courville A, Bengio Y. Generative adversarial nets. \\nIn: Advances in neural information processing systems. 2014; \\np. 2672–680.\\n 33. Google trends. 2021. https:// trends. google. com/ trends/.\\n 34. Gruber N, Jockisch A. Are gru cells more specific and lstm \\ncells more sensitive in motive classification of text? Front Artif \\nIntell. 2020;3:40.\\n 35. Gu B, Ge R, Chen Y, Luo L, Coatrieux G. Automatic and \\nrobust object detection in x-ray baggage inspection using deep \\nconvolutional neural networks. IEEE Trans Ind Electron. 2020.\\n 36. Han J, Pei J, Kamber M. Data mining: concepts and techniques. \\nAmsterdam: Elsevier; 2011.\\n 37. Haykin S. Neural networks and learning machines, 3/E. Lon-\\ndon: Pearson Education; 2010.\\n 38. He K, Zhang X, Ren S, Sun J. Spatial pyramid pooling in deep \\nconvolutional networks for visual recognition. IEEE Trans Pat-\\ntern Anal Mach Intell. 2015;37(9):1904–16.\\n 39. He K, Zhang X, Ren S, Sun J. Deep residual learning for image \\nrecognition. In: Proceedings of the IEEE Conference on com-\\nputer vision and pattern recognition, 2016; p. 770–78.\\n 40. Hinton GE. Deep belief networks. Scholarpedia. \\n2009;4(5):5947.\\n 41. Hinton GE, Osindero S, Teh Y-W. A fast learning algorithm \\nfor deep belief nets. Neural Comput. 2006;18(7):1527–54.\\n 42. Hochreiter S, Schmidhuber J. Long short-term memory. Neural \\nComput. 1997;9(8):1735–80.\\n 43. Huang C-J, Kuo P-H. A deep cnn-lstm model for particu-\\nlate matter (pm2. 5) forecasting in smart cities. Sensors. \\n2018;18(7):2220.\\n 44. Huang H-H, Fukuda M, Nishida T. Toward rnn based micro \\nnon-verbal behavior generation for virtual listener agents. In: \\nInternational Conference on human-computer interaction, \\n2019; p. 53–63. Springer.\\n 45. Hulsebos M, Hu K, Bakker M, Zgraggen E, Satyanarayan A, \\nKraska T, Demiralp Ça, Hidalgo C. Sherlock: a deep learning \\napproach to semantic data type detection. In: Proceedings of \\nthe 25th ACM SIGKDD International Conference on knowl-\\nedge discovery & data mining, 2019; p. 1500–508. 46. Imamverdiyev Y, Abdullayeva F. Deep learning method for \\ndenial of service attack detection based on restricted Boltzmann \\nmachine. Big Data. 2018;6(2):159–69.\\n 47. Islam MZ, Islam MM, Asraf A. A combined deep cnn-lstm net-\\nwork for the detection of novel coronavirus (covid-19) using \\nx-ray images. Inf Med Unlock. 2020;20:100412.\\n 48. Ismail WN, Hassan MM, Alsalamah HA, Fortino G. Cnn-based \\nhealth model for regular health factors analysis in internet-of-\\nmedical things environment. IEEE. Access. 2020;8:52541–9.\\n 49. Jangid H, Singhal S, Shah RR, Zimmermann R. Aspect-based \\nfinancial sentiment analysis using deep learning. In: Compan-\\nion Proceedings of the The Web Conference 2018, 2018; p. \\n1961–966.\\n 50. Kaelbling LP, Littman ML, Moore AW. Reinforcement learning: \\na survey. J Artif Intell Res. 1996;4:237–85.\\n 51. Kameoka H, Li L, Inoue S, Makino S. Supervised determined \\nsource separation with multichannel variational autoencoder. \\nNeural Comput. 2019;31(9):1891–914.\\n 52. Karhunen J, Raiko T, Cho KH. Unsupervised deep learning: a \\nshort review. In: Advances in independent component analysis \\nand learning machines. 2015; p. 125–42.\\n 53. Kawde P, Verma GK. Deep belief network based affect recogni-\\ntion from physiological signals. In: 2017 4th IEEE Uttar Pradesh \\nSection International Conference on electrical, computer and \\nelectronics (UPCON), 2017; p. 587–92. IEEE.\\n 54. Kim J-Y, Seok-Jun B, Cho S-B. Zero-day malware detection \\nusing transferred generative adversarial networks based on deep \\nautoencoders. Inf Sci. 2018;460:83–102.\\n 55. Kingma DP, Welling M. Auto-encoding variational bayes. arXiv \\npreprint arXiv:1312.6114, 2013.\\n 56. Kingma DP, Welling M. An introduction to variational autoen-\\ncoders. arXiv preprint arXiv:1906.02691, 2019.\\n 57. Kiran PKR, Bhasker B. Dnnrec: a novel deep learning based \\nhybrid recommender system. Expert Syst Appl. 2020.\\n 58. Kloenne M, Niehaus S, Lampe L, Merola A, Reinelt J, Roeder \\nI, Scherf N. Domain-specific cues improve robustness of \\ndeep learning-based segmentation of ct volumes. Sci Rep. \\n2020;10(1):1–9.\\n 59. Kohonen T. The self-organizing map. Proc IEEE. \\n1990;78(9):1464–80.\\n 60. Kohonen T. Essentials of the self-organizing map. Neural Netw. \\n2013;37:52–65.\\n 61. Kök İ, Şimşek MU, Özdemir S. A deep learning model for air \\nquality prediction in smart cities. In: 2017 IEEE International \\nConference on Big Data (Big Data), 2017; p. 1983–990. IEEE.\\n 62. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification \\nwith deep convolutional neural networks. In: Advances in neural \\ninformation processing systems. 2012; p. 1097–105.\\n 63. Latif S, Rana R, Younis S, Qadir J, Epps J. Transfer learning for \\nimproving speech emotion classification accuracy. arXiv preprint \\narXiv:1801.06353, 2018.\\n 64. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. \\n2015;521(7553):436–44.\\n 65. LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based \\nlearning applied to document recognition. Proc IEEE. \\n1998;86(11):2278–324.\\n 66. Li B, François-Lavet V, Doan T, Pineau J. Domain adversarial \\nreinforcement learning. arXiv preprint arXiv:2102.07097, 2021.\\n 67. Li T-HS, Kuo P-H, Tsai T-N, Luan P-C. Cnn and lstm based \\nfacial expression analysis model for a humanoid robot. IEEE \\nAccess. 2019;7:93998–4011.\\n 68. Liu C, Cao Y, Luo Y, Chen G, Vokkarane V, Yunsheng M, Chen \\nS, Hou P. A new deep learning-based food recognition system for \\ndietary assessment on an edge computing service infrastructure. \\nIEEE Trans Serv Comput. 2017;11(2):249–61.\\nSN Computer Science (2021) 2:420 \\n Page 19 of 20 420\\nSN Computer Science\\n 69. Liu W, Wang Z, Liu X, Zeng N, Liu Y, Alsaadi FE. A survey of \\ndeep neural network architectures and their applications. Neuro-\\ncomputing. 2017;234:11–26.\\n 70. López AU, Mateo F, Navío-Marco J, Martínez-Martínez JM, \\nGómez-Sanchís J, Vila-Francés J, Serrano-López AJ. Analysis \\nof computer user behavior, security incidents and fraud using \\nself-organizing maps. Comput Secur. 2019;83:38–51.\\n 71. Lopez-Martin M, Carro B, Sanchez-Esguevillas A. Application \\nof deep reinforcement learning to intrusion detection for super -\\nvised problems. Expert Syst Appl. 2020;141:112963.\\n 72. Ma X, Yao T, Menglan H, Dong Y, Liu W, Wang F, Liu J. A sur -\\nvey on deep learning empowered iot applications. IEEE Access. \\n2019;7:181721–32.\\n 73. Makhzani A, Frey B. K-sparse autoencoders. arXiv preprint \\narXiv:1312.5663, 2013.\\n 74. Mandic D, Chambers J. Recurrent neural networks for prediction: \\nlearning algorithms, architectures and stability. Hoboken: Wiley; \\n2001.\\n 75. Marlin B, Swersky K, Chen B, Freitas N. Inductive principles \\nfor restricted boltzmann machine learning. In: Proceedings of the \\nThirteenth International Conference on artificial intelligence and \\nstatistics, p. 509–16. JMLR Workshop and Conference Proceed-\\nings, 2010.\\n 76. Masud M, Muhammad G, Alhumyani H, Alshamrani SS, \\nCheikhrouhou O, Ibrahim S, Hossain MS. Deep learning-based \\nintelligent face recognition in iot-cloud environment. Comput \\nCommun. 2020;152:215–22.\\n 77. Memisevic R, Hinton GE. Learning to represent spatial transfor -\\nmations with factored higher-order boltzmann machines. Neural \\nComput. 2010;22(6):1473–92.\\n 78. Minaee S, Azimi E, Abdolrashidi AA. Deep-sentiment: senti-\\nment analysis using ensemble of cnn and bi-lstm models. arXiv \\npreprint arXiv:1904.04206, 2019.\\n 79. Naeem M, Paragliola G, Coronato A. A reinforcement learn-\\ning and deep learning based intelligent system for the support \\nof impaired patients in home treatment. Expert Syst Appl. \\n2021;168:114285.\\n 80. Niu Z, Zhong G, Hui Yu. A review on the attention mechanism \\nof deep learning. Neurocomputing. 2021;452:48–62.\\n 81. Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans \\nKnowl Data Eng. 2009;22(10):1345–59.\\n 82. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, \\nKilleen T, Lin Z, Gimelshein N, Antiga L, et\\xa0al. Pytorch: An \\nimperative style, high-performance deep learning library. Adv \\nNeural Inf Process Syst. 2019;32:8026–37.\\n 83. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, \\nGrisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, et\\xa0al. \\nScikit-learn: machine learning in python. J Mach Learn Res. \\n2011;12:2825–30.\\n 84. Pi Y, Nath ND, Behzadan AH. Convolutional neural networks \\nfor object detection in aerial imagery for disaster response and \\nrecovery. Adv Eng Inf. 2020;43:101009.\\n 85. Piccialli F, Giampaolo F, Prezioso E, Crisci D, Cuomo S. Pre-\\ndictive analytics for smart parking: A deep learning approach \\nin forecasting of iot data. ACM Trans Internet Technol (TOIT). \\n2021;21(3):1–21.\\n 86. Puterman ML. Markov decision processes: discrete stochastic \\ndynamic programming. Hoboken: Wiley; 2014.\\n 87. Qu X, Lin Y, Kai G, Linru M, Meng S, Mingxing K, Mu L, \\neditors. A survey on the development of self-organizing maps \\nfor unsupervised intrusion detection. Mob Netw Appl. 2019; p. \\n1–22.\\n 88. Rahman MW, Tashfia SS, Islam R, Hasan MM, Sultan SI, Mia \\nS, Rahman MM. The architectural design of smart blind assistant using iot with deep learning paradigm. Internet of Things. \\n2021;13:100344.\\n 89. Ren J, Green M, Huang X. From traditional to deep learning: \\nfault diagnosis for autonomous vehicles. In: Learning control. \\nElsevier. 2021; p. 205–19.\\n 90. Rifai S, Vincent P, Muller X, Glorot X, Bengio Y. Contractive \\nauto-encoders: Explicit invariance during feature extraction. \\nIn: Icml, 2011.\\n 91. Rosa RL, Schwartz GM, Ruggiero WV, Rodríguez DZ. A \\nknowledge-based recommendation system that includes \\nsentiment analysis and deep learning. IEEE Trans Ind Inf. \\n2018;15(4):2124–35.\\n 92. Sarker IH. Context-aware rule learning from smartphone \\ndata: survey, challenges and future directions. J Big Data. \\n2019;6(1):1–25.\\n 93. Sarker IH. A machine learning based robust prediction model for \\nreal-life mobile phone data. Internet of Things. 2019;5:180–93.\\n 94. Sarker IH. Cyberlearning: effectiveness analysis of machine \\nlearning security modeling to detect cyber-anomalies and multi-\\nattacks. Internet of Things. 2021;14:100393.\\n 95. Sarker IH. Data science and analytics: an overview from data-\\ndriven smart computing, decision-making and applications per -\\nspective. SN Comput Sci. 2021.\\n 96. Sarker IH. Deep cybersecurity: a comprehensive overview from \\nneural network and deep learning perspective. SN Computer. \\nScience. 2021;2(3):1–16.\\n 97. Sarker IH. Machine learning: Algorithms, real-world applications \\nand research directions. SN Computer. Science. 2021;2(3):1–21.\\n 98. Sarker IH, Abushark YB, Alsolami F, Khan AI. Intrudtree: a \\nmachine learning based cyber security intrusion detection model. \\nSymmetry. 2020;12(5):754.\\n 99. Sarker IH, Abushark YB, Khan AI. Contextpca: Predicting con-\\ntext-aware smartphone apps usage based on machine learning \\ntechniques. Symmetry. 2020;12(4):499.\\n 100. Sarker IH, Colman A, Han J. Recencyminer: mining recency-\\nbased personalized behavior from contextual smartphone data. J \\nBig Data. 2019;6(1):1–21.\\n 101. Sarker IH, Colman A, Han J, Khan AI, Abushark YB, Salah \\nK. Behavdt: a behavioral decision tree learning to build user-\\ncentric context-aware predictive model. Mob Netw Appl. \\n2020;25(3):1151–61.\\n 102. Sarker IH, Colman A, Kabir MA, Han J. Individualized time-\\nseries segmentation for mining mobile phone user behavior. \\nComput J. 2018;61(3):349–68.\\n 103. Sarker IH, Furhad MH, Nowrozy R. Ai-driven cybersecurity: an \\noverview, security intelligence modeling and research directions. \\nSN Computer. Science. 2021;2(3):1–18.\\n 104. Sarker IH, Hoque MM, Uddin MK. Mobile data science and \\nintelligent apps: concepts, ai-based modeling and research direc-\\ntions. Mob Netw Appl. 2021;26(1):285–303.\\n 105. Sarker IH, Kayes ASM. Abc-ruleminer: User behavioral rule-\\nbased machine learning method for context-aware intelligent \\nservices. J Netw Comput Appl. 2020;168:102762.\\n 106. Sarker IH, Kayes ASM, Badsha S, Alqahtani H, Watters P, Ng A. \\nCybersecurity data science: an overview from machine learning \\nperspective. J Big data. 2020;7(1):1–29.\\n 107. Sarker IH, Kayes ASM, Watters P. Effectiveness analy -\\nsis of machine learning classification models for predicting \\npersonalized context-aware smartphone usage. J Big Data. \\n2019;6(1):1–28.\\n 108. Sarker IH, Salah K. Appspred: predicting context-aware smart-\\nphone apps using random forest learning. Internet of Things. \\n2019;8:100106.\\n SN Computer Science (2021) 2:420\\n 420 Page 20 of 20\\nSN Computer Science\\n 109. Satt A, Rozenberg S, Hoory R. Efficient emotion recognition \\nfrom speech using deep learning on spectrograms. In: Interspeec, \\n2017; p. 1089–1093.\\n 110. Sevakula RK, Singh V, Verma NK, Kumar C, Cui Y. Trans-\\nfer learning for molecular cancer classification using deep \\nneural networks. IEEE/ACM Trans Comput Biol Bioinf. \\n2018;16(6):2089–100.\\n 111. Sujay Narumanchi H, Ananya Pramod\\xa0Kompalli Shankar A, \\nDevashish CK. Deep learning based large scale visual rec-\\nommendation and search for e-commerce. arXiv preprint \\narXiv:1703.02344, 2017.\\n 112. Shao X, Kim CS. Multi-step short-term power consumption fore-\\ncasting using multi-channel lstm with time location considering \\ncustomer behavior. IEEE Access. 2020;8:125263–73.\\n 113. Siami-Namini S, Tavakoli N, Namin AS. The performance of \\nlstm and bilstm in forecasting time series. In: 2019 IEEE Inter -\\nnational Conference on Big Data (Big Data), 2019; p. 3285–292. \\nIEEE.\\n 114. Ślusarczyk B. Industry 4.0: are we ready? Pol J Manag Stud. \\n2018; p. 17\\n 115. Sumathi P, Subramanian R, Karthikeyan VV, Karthik S. Soil \\nmonitoring and evaluation system using edl-asqe: enhanced deep \\nlearning model for ioi smart agriculture network. Int J Commun \\nSyst. 2021; p. e4859.\\n 116. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan \\nD, Vanhoucke V, Rabinovich A. Going deeper with convolutions. \\nIn: Proceedings of the IEEE Conference on computer vision and \\npattern recognition, 2015; p. 1–9.\\n 117. Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A survey on \\ndeep transfer learning. In: International Conference on artificial \\nneural networks, 2018; p. 270–279. Springer.\\n 118. Vesanto J, Alhoniemi E. Clustering of the self-organizing map. \\nIEEE Trans Neural Netw. 2000;11(3):586–600.\\n 119. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol P-A, \\nBottou L. Stacked denoising autoencoders: Learning useful rep-\\nresentations in a deep network with a local denoising criterion. \\nJ Mach Learn Res. 2010;11(12).\\n 120. Wang J, Liang-Chih Yu, Robert Lai K, Zhang X. Tree-structured \\nregional cnn-lstm model for dimensional sentiment analysis. \\nIEEE/ACM Trans Audio Speech Lang Process. 2019;28:581–91.\\n 121. Wang S, Wan J, Li D, Liu C. Knowledge reasoning with seman-\\ntic data for real-time data processing in smart factory. Sensors. \\n2018;18(2):471.\\n 122. Wang W, Zhao M, Wang J. Effective android malware detec-\\ntion with a hybrid model based on deep autoencoder and con-\\nvolutional neural network. J Ambient Intell Humaniz Comput. \\n2019;10(8):3035–43. 123. Wang X, Liu J, Qiu T, Chaoxu M, Chen C, Zhou P. A real-\\ntime collision prediction mechanism with deep learning for \\nintelligent transportation system. IEEE Trans Veh Technol. \\n2020;69(9):9497–508.\\n 124. Wang Y, Huang M, Zhu X, Zhao L. Attention-based lstm for \\naspect-level sentiment classification. In: Proceedings of the 2016 \\nConference on empirical methods in natural language processing, \\n2016; p. 606–615.\\n 125. Wei P, Li Y, Zhang Z, Tao H, Li Z, Liu D. An optimization \\nmethod for intrusion detection classification model based on deep \\nbelief network. IEEE Access. 2019;7:87593–605.\\n 126. Weiss K, Khoshgoftaar TM, Wang DD. A survey of transfer \\nlearning. J Big data. 2016;3(1):9.\\n 127. Xin Y, Kong L, Liu Z, Chen Y, Li Y, Zhu H, Gao M, Hou H, \\nWang C. Machine learning and deep learning methods for cyber -\\nsecurity. Ieee access. 2018;6:35365–81.\\n 128. Xu W, Sun H, Deng C, Tan Y. Variational autoencoder for semi-\\nsupervised text classification. In: Thirty-First AAAI Conference \\non artificial intelligence, 2017.\\n 129. Xue Q, Chuah MC. New attacks on rnn based healthcare learning \\nsystem and their detections. Smart Health. 2018;9:144–57.\\n 130. Yousefi-Azar M, Hamey L. Text summarization using unsuper -\\nvised deep learning. Expert Syst Appl. 2017;68:93–105.\\n 131. Yuan X, Shi J, Gu L. A review of deep learning methods for \\nsemantic segmentation of remote sensing imagery. Expert Syst \\nAppl. 2020;p. 114417.\\n 132. Zhang G, Liu Y, Jin X. A survey of autoencoder-based recom-\\nmender systems. Front Comput Sci. 2020;14(2):430–50.\\n 133. Zhang X, Yao L, Huang C, Wang S, Tan M, Long Gu, Wang C. \\nMulti-modality sensor data classification with selective attention. \\narXiv preprint arXiv:1804.05493, 2018.\\n 134. Zhang X, Yao L, Wang X, Monaghan J, Mcalpine D, Zhang Y. A \\nsurvey on deep learning based brain computer interface: recent \\nadvances and new frontiers. arXiv preprint arXiv:1905.04149, \\n2019; p. 66.\\n 135. Zhang Y, Zhang P, Yan Y. Attention-based lstm with multi-task \\nlearning for distant speech recognition. In: Interspeech, 2017; p. \\n3857–861.\\nPublisher's Note Springer Nature remains neutral with regard to \\njurisdictional claims in published maps and institutional affiliations.\\nView publication stats\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)"
      ],
      "metadata": {
        "id": "LbuMoJJ-_kQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "qRdZvgDi_nKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMu6RI_R_8j4",
        "outputId": "37842eb7-cefb-4762-9cea-5fc13e9b30d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "220"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEzHFj_tAAya",
        "outputId": "22cff139-0aad-40de-8174-4540c5f402b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/353986944\n",
            "Deep Learning: A Comprehensive Overview on Techniques, Taxonomy,\n",
            "Applications and Research Directions\n",
            "Article    in  SN Comput er Scienc e · August 2021\n",
            "DOI: 10.1007/s42979-021-00815-1\n",
            "CITATIONS\n",
            "1,195READS\n",
            "5,252\n",
            "1 author:\n",
            "Iqbal H. Sark er\n",
            "Edith Co wan Univ ersity\n",
            "235 PUBLICA TIONS    10,044  CITATIONS    \n",
            "SEE PROFILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGh-E7pCAK_J",
        "outputId": "5b34c0f8-0efc-4b92-82b1-949a30bd6c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235 PUBLICA TIONS    10,044  CITATIONS    \n",
            "SEE PROFILE\n",
            "All c ontent f ollo wing this p age was uplo aded b y Iqbal H. Sark er on 13 Mar ch 2024.\n",
            "The user has r equest ed enhanc ement of the do wnlo aded file.\n",
            "Vol.:(0123456789)SN Computer Science (2021) 2:420 \n",
            "https://doi.org/10.1007/s42979-021-00815-1\n",
            "SN Computer Science\n",
            "REVIEW ARTICLE\n",
            "Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, \n",
            "Applications and Research Directions\n",
            "Iqbal H. Sarker1,2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqj_wXwS9kKG",
        "outputId": "d9376157-2458-4d7a-c9ce-154c95c955e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14237"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ChromaDB requires us to have documents with two fields:\n",
        "# page_content -- this is the document\n",
        "# metadata -- not used, but we can use it for filtering if needed\n",
        "class ChromaShip:\n",
        "    def __init__(self, content, metadata=None):\n",
        "        self.page_content: str = content\n",
        "        self.metadata = metadata\n",
        "\n",
        "# split the data into chunks, make sure the title is used with every chunk\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
        "\n",
        "documents = []\n",
        "documents.extend([ChromaShip(content=doc, metadata=None) for doc in docs])\n"
      ],
      "metadata": {
        "id": "A9oWax0D9vJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].page_content"
      ],
      "metadata": {
        "id": "7wHuohBrGjSK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0dcedea8-0e7d-4888-d204-a7756ef82e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/353986944\\nDeep Learning: A Comprehensive Overview on Techniques, Taxonomy,\\nApplications and Research Directions\\nArticle \\xa0\\xa0 in\\xa0\\xa0SN Comput er Scienc e · August 2021\\nDOI: 10.1007/s42979-021-00815-1\\nCITATIONS\\n1,195READS\\n5,252\\n1 author:\\nIqbal H. Sark er\\nEdith Co wan Univ ersity\\n235 PUBLICA TIONS \\xa0\\xa0\\xa010,044  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-large\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "db = Chroma.from_documents(documents, embeddings, persist_directory=\"db_njscuba\")"
      ],
      "metadata": {
        "id": "D_SnLfTuuF6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "e6795ca281c04336bc849c06eaa5e88f",
            "954205745d94452dbbe160bf08cef6f0",
            "4e34056c9be14cd885e78bba565335a7",
            "35a92d08199a4ebc939c689f08699ea6",
            "1368912925fd41ec82cdd067650d2fd0",
            "373f4b91025243ad826e3a3789ac3a4b",
            "1344293d47a54a62bda2d2855dd63efc",
            "e85abde34ce9411583d9a6d8f7835047",
            "b3ae9e26d58a453ba08d4bdd5be58745",
            "5220d8a46d1a437cb41d0b79cc38973d",
            "243ea69df24d444f877dc4e43e964480",
            "d3699b7f269a4902900c5ccc29e994dc",
            "3105b7334b52405ba2f3f06e6fba7131",
            "9ce887b659c54bb78cea5844dd6520fe",
            "20d2d84c08aa4a5a994643a9b765664d",
            "0e916adaf1ec4eda8fa0d7ee1df5ff00",
            "127c20c0824740cf865bfc10a4d535d7",
            "9cba357d89834c4fb4713b559935b0ec",
            "6d64d29edd504ef599b5d88896a2c05c",
            "60ea7012d0c24ba19cddf4c402be16c3",
            "4d286204a62e444aa9a0f176275ec4ae",
            "0514870763434485a1ff2ff40d413f3b",
            "82c4c115e6d04cb6a8fc86657202922d",
            "2f196c5e4caa4f308d8701200793c4c0",
            "23f5d63eb57b423eb16fac508bea2351",
            "36dc3485bb8148aeaf1beca3f43f06b5",
            "5f903098113c44e7ba223cc78b651092",
            "dee26a294635435aa92c7556402171ff",
            "69c09c15dee34e11b5159fcddea53871",
            "067086bde7464b9598045cfef1bfccff",
            "21b72b5d9a0b42c2a4fd2b7f109200a8",
            "0937ac5f8e77472b825b4487d2cb6b14",
            "a5aec4827c3b443b9013ca9a11013e0a",
            "07f26ef774e84a31a801bff7f925720e",
            "7af29169e5ff4823b57e178942d62dab",
            "db975fd013a6417d915ed12316818829",
            "da7767574d5949f58b1a70d6d1ed046c",
            "28917f2378fb40b5b5517c346a83cd85",
            "1b98b8411b004781ab887cdf43f9153d",
            "92c3e0f091ee428dbece89130fe7f217",
            "0f4568c95ae74fc4b737e07bc303e342",
            "fb405200b891483498048973e73f11f0",
            "34d20b84780d4c379c8df8a75f9fdec2",
            "a448e189940142bab64c7c642a0837a4",
            "dee905d09a23418b9a0930d3e8be2a44",
            "03fc51ed5ace421c8753d9b5c577b39d",
            "2b1a52e9d2674f8394de7a463d33b9c6",
            "33a9343e4f024d92afdb540a1b9cd97f",
            "fd0e4eb5ddaf457eb5a804381a95bb4c",
            "75d1c901431741bcb3a1463e1a0a40a4",
            "db233e7c38e4435b8ef5d9849270ba11",
            "6a3053e26a9045c188b39172445db0ca",
            "9d0460ae1e3043b69a61b4f29a55e625",
            "72f59c22f2024efdad1b98e9994cd96a",
            "82f1948b41b04bb082c338283c23c74e",
            "7b7b8ade66594cd7af7abd64ce3ddc98",
            "41dca18c157b4600a6ae64e66f1c331e",
            "69df73d7db374f0f8dd5debb76b3d7e1",
            "8a142a430fce4fb9a9a20dac0118dc66",
            "67bd389a36044d9c947a39aad6de948a",
            "bdf23a831dd843dc90064149fd52c8b7",
            "b718d550df5043058c950a8b007eebfe",
            "eb4c24f28cf748abba0b192676c95960",
            "9af9f09d9c214ab499333073358078b8",
            "30d1cfccf9794d5fb63abb4ff881372c",
            "f662a0be9508413f8e43eb62dede4e2f",
            "ad446d725b344d92ba3aaed2a28e1196",
            "7f6d077d69534777b6514bc7adc61a53",
            "8c108e5dab2249e7a728c75d34d50d1c",
            "5c69977484ff463ea1ecfd091581db35",
            "a5e02dec50164e7c8c8f91f16d1ab103",
            "bc4fc9a691314c0eb29fb66bf8bd1c29",
            "d853823e15774f099b2c7a4ba6a6bcc7",
            "6a23ad0612334099ac863350e490158a",
            "ae0cfaf4c8414a5987e4ac739d6d4a94",
            "a4e526c064324623849edae03ba80e3d",
            "ecda8314ba2c4cb7a30e7601aebc17a6",
            "10e0f53e53e446519923441940614852",
            "41b23af96644426da7cf327329fcbcc3",
            "89570f382079458aacb988c56f6b517c",
            "7b52fb85164941fbaf871f493638aba7",
            "dcf3ece4983645bead0865b32d506d56",
            "8573417e2ce743db8de78a8965541163",
            "2eb5874fdbad47368014b4acacba82e5",
            "a81df18a6e3c43b98b9ff6da220f4bcd",
            "a3cda969ae2b41dca2fc1e78328d49b3",
            "e0c4f21595ca4fe7b23716642167269e",
            "b990b6d841c747158fb8c26524cd265d",
            "4157e28438464d1d96a60c617c1b7fe6",
            "e4c950b1526d40c2b3f14dc7d677037f",
            "41c2806ce7ca4546ab5f7d6357ebd6aa",
            "d6a58b145587491e86a8a4205d3f2dec",
            "eab1ea524d58474d9336b244050a9e1b",
            "a0fe74d2c7dc45bd8ea56229a2053a21",
            "2753e05e17844aaaa2842a742017c877",
            "5f7271d056604a228a09904c3076e3a2",
            "bd88ad3e5cf24cd3b6cae39700a945ea",
            "c37a56f1a27f4ee6a78fd40550b20708",
            "c1c1bb689e084bc49a632b6fb2f4ec5a",
            "8a68ec80a7f248b9816178a57432967d",
            "d3ee13e682834a158eef56a9c43aba23",
            "d84e9298b8724944be256cf844b59bde",
            "f490d8abdc784a7d9be9597e402e0526",
            "6198fdd38c6a4217a3e269404c5da6f9",
            "87da87c0c5954605800acf79ea238186",
            "3e79cb41295a40ffb9da01714f23b65d",
            "8f7a629f1a0a46168a43c8a09b5ff8c8",
            "aa327a67c4f5495a9cc508d124f18edd",
            "7e9df48e9c7a4a828ca809c47fa5363e",
            "cf47c3439bc3459286c9da75df5bd2f9"
          ]
        },
        "outputId": "e2b7b692-f724-4028-ffd8-685d8018da82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6795ca281c04336bc849c06eaa5e88f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3699b7f269a4902900c5ccc29e994dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82c4c115e6d04cb6a8fc86657202922d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07f26ef774e84a31a801bff7f925720e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dee905d09a23418b9a0930d3e8be2a44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b7b8ade66594cd7af7abd64ce3ddc98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad446d725b344d92ba3aaed2a28e1196"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10e0f53e53e446519923441940614852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4157e28438464d1d96a60c617c1b7fe6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a68ec80a7f248b9816178a57432967d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTCWGO7uGUnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.similarity_search_with_score(\"what is Backpropagation?\",3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSof56WzDYbl",
        "outputId": "8d8224d8-d8c8-47be-a00d-6aa99df7ea8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={}, page_content='(ii) deep networks for unsupervised or generative learning, and (ii) deep networks for hybrid learning and relevant others\\nSN Computer Science (2021) 2:420 \\n Page 7 of 20 420\\nSN Computer Science\\nused algorithm “Backpropagation” [36], a supervised learn-\\ning technique, which is also known as the most basic build-\\ning block of a neural network. During the training process, \\nvarious optimization approaches such as Stochastic Gradi-\\nent Descent (SGD), Limited Memory BFGS (L-BFGS), and'),\n",
              "  0.2124413102865219),\n",
              " (Document(metadata={}, page_content='tecture of deep neural networks (DNN) or deep learning. A \\ntypical MLP is a fully connected network that consists of \\nan input layer that receives input data, an output layer that \\nmakes a decision or prediction about the input signal, and \\none or more hidden layers between these two that are consid-\\nered as the network’s computational engine [ 36, 103]. The \\noutput of an MLP network is determined using a variety of \\nactivation functions, also known as transfer functions, such'),\n",
              "  0.2837468981742859),\n",
              " (Document(metadata={}, page_content='activation functions, also known as transfer functions, such \\nas ReLU (Rectified Linear Unit), Tanh, Sigmoid, and Soft-\\nmax [83, 96]. To train MLP employs the most extensively \\nFig. 5  A general architecture of a a shallow network with one hidden layer and b a deep neural network with multiple hidden layers\\nFig. 6  A taxonomy of DL techniques, broadly divided into three major categories (i) deep networks for supervised or discriminative learning,'),\n",
              "  0.29756075143814087)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(map(lambda x : x.page_content,db.similarity_search(\"Tell me about Hardware Dependencies in Deep Learning\",3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZQo4ovZEQKa",
        "outputId": "e716bb0c-1698-43e5-e924-92f5be6c6898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[64, 107].\\n– Hardware Dependencies The DL algorithms require \\nlarge computational operations while training a model \\nwith large datasets. As the larger the computations, the \\nmore the advantage of a GPU over a CPU, the GPU is \\nmostly used to optimize the operations efficiently. Thus, \\nto work properly with the deep learning training, GPU \\nhardware is necessary. Therefore, DL relies more on \\nhigh-performance machines with GPUs than standard \\nmachine learning methods [19, 127].',\n",
              " 'modeling for real-world applications.\\n– Data Dependencies  Deep learning is typically dependent \\non a large amount of data to build a data-driven model \\nfor a particular problem domain. The reason is that when \\nthe data volume is small, deep learning algorithms often \\nperform poorly [64 ]. In such circumstances, however, \\nthe performance of the standard machine-learning algo-\\nrithms will be improved if the specified rules are used \\n[64, 107].\\n– Hardware Dependencies The DL algorithms require',\n",
              " 'monly used in various application areas [97]. On the other \\nhand, the DL model includes convolution neural network, \\nrecurrent neural network, autoencoder, deep belief network, \\nand many more, discussed briefly with their potential appli-\\ncation areas in Section\\xa0 3. In the following, we discuss the \\nkey properties and dependencies of DL techniques, that are \\nFig. 3  An illustration of the performance comparison between deep \\nlearning (DL) and other machine learning (ML) algorithms, where']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes-cuda110 bitsandbytes\n"
      ],
      "metadata": {
        "id": "HsKJOEAiwGWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "6TAD39TuwUAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart"
      ],
      "metadata": {
        "id": "uJF9aC4XxHpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Mistral7B\n",
        "# resource link: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ],
      "metadata": {
        "id": "WbTo1D4JvJyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")"
      ],
      "metadata": {
        "id": "pa5--OsC6ALc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=2048,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# specify the llm\n"
      ],
      "metadata": {
        "id": "MMc_8oqAz14q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "TJnEfx8E7Y1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "68GklUS4wxEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_chain.run(\n",
        "    \"what is “Backpropagation”?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "elVEfL1Iw2IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about Hardware Dependencies in Deep Learning\"\n",
        "result = llm(query)\n",
        "\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))"
      ],
      "metadata": {
        "id": "4lOFI7efvJtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p8jUfQGGBHMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
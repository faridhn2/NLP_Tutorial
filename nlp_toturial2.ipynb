{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGU97sSK0K8z"
      },
      "source": [
        "# TensorFlow Tutorial #20\n",
        "# Natural Language Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37yzMNKW0K82"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This tutorial is about a basic form of Natural Language Processing (NLP) called Sentiment Analysis, in which we will try and classify a movie review as either positive or negative.\n",
        "\n",
        "Consider a simple example: \"This movie is not very good.\" This text ends with the words \"very good\" which indicates a very positive sentiment, but it is negated because it is preceded by the word \"not\", so the text should be classified as having a negative sentiment. How can we teach a Neural Network to do this classification?\n",
        "\n",
        "Another problem is that neural networks cannot work directly on text-data, so we need to convert text into numbers that are compatible with a neural network.\n",
        "\n",
        "Yet another problem is that a text may be arbitrarily long. The neural networks we have worked with in previous tutorials use fixed data-shapes - except for the first dimension of the data which varies with the batch-size. Now we need a type of neural network that can work on both short and long sequences of text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8kaON4r0K83"
      },
      "source": [
        "## Flowchart\n",
        "\n",
        "To solve this problem we need several processing steps. First we need to convert the raw text-words into so-called tokens which are integer values. These tokens are really just indices into a list of the entire vocabulary. Then we convert these integer-tokens into so-called embeddings which are real-valued vectors, whose mapping will be trained along with the neural network, so as to map words with similar meanings to similar embedding-vectors. Then we input these embedding-vectors to a Recurrent Neural Network which can take sequences of arbitrary length as input and output a kind of summary of what it has seen in the input. This output is then squashed using a Sigmoid-function to give us a value between 0.0 and 1.0, where 0.0 is taken to mean a negative sentiment and 1.0 means a positive sentiment. This whole process allows us to classify input-text as either having a negative or positive sentiment.\n",
        "\n",
        "The flowchart of the algorithm is roughly:\n",
        "\n",
        "<img src=\"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_natural_language_flowchart.png?raw=1\" alt=\"Flowchart NLP\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWiFTHEn0K84"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "The basic building block in a Recurrent Neural Network (RNN) is a Recurrent Unit (RU). There are many different variants of recurrent units such as the rather clunky LSTM (Long-Short-Term-Memory) and the somewhat simpler GRU (Gated Recurrent Unit) which we will use in this tutorial. Experiments in the literature suggest that the LSTM and GRU have roughly similar performance. Even simpler variants also exist and the literature suggests that they may perform even better than both LSTM and GRU, but they are not implemented in Keras which we will use in this tutorial.\n",
        "\n",
        "The following figure shows the abstract idea of a recurrent unit, which has an internal state that is being updated every time the unit receives a new input. This internal state serves as a kind of memory. However, it is not a traditional kind of computer memory which stores bits that are either on or off. Instead the recurrent unit stores floating-point values in its memory-state, which are read and written using matrix-operations so the operations are all differentiable. This means the memory-state can store arbitrary floating-point values (although typically limited between -1.0 and 1.0) and the network can be trained like a normal neural network using Gradient Descent.\n",
        "\n",
        "The new state-value depends on both the old state-value and the current input. For example, if the state-value has memorized that we have recently seen the word \"not\" and the current input is \"good\" then we need to store a new state-value that memorizes \"not good\" which indicates a negative sentiment.\n",
        "\n",
        "The part of the recurrent unit that is responsible for mapping old state-values and inputs to the new state-value is called a gate, but it is really just a type of matrix-operation. There is another gate for calculating the output-values of the recurrent unit. The implementation of these gates vary for different types of recurrent units. This figure merely shows the abstract idea of a recurrent unit. The LSTM has more gates than the GRU but some of them are apparently redundant so they can be omitted.\n",
        "\n",
        "In order to train the recurrent unit, we must gradually change the weight-matrices of the gates so the recurrent unit gives the desired output for an input sequence. This is done automatically in TensorFlow.\n",
        "\n",
        "![Recurrent unit](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_recurrent_unit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs2kkSN70K85"
      },
      "source": [
        "### Unrolled Network\n",
        "\n",
        "Another way to visualize and understand a Recurrent Neural Network is to \"unroll\" the recursion. In this figure there is only a single recurrent unit denoted RU, which will receive a text-word from the input sequence in a series of time-steps.\n",
        "\n",
        "The initial memory-state of the RU is reset to zero internally by Keras / TensorFlow every time a new sequence begins.\n",
        "\n",
        "In the first time-step the word \"this\" is input to the RU which uses its internal state (initialized to zero) and its gate to calculate the new state. The RU also uses its other gate to calculate the output but it is ignored here because it is only needed at the end of the sequence to output a kind of summary.\n",
        "\n",
        "In the second time-step the word \"is\" is input to the RU which now uses the internal state that was just updated from seeing the previous word \"this\".\n",
        "\n",
        "There is not much meaning in the words \"this is\" so the RU probably doesn't save anything important in its internal state from seeing these words. But when it sees the third word \"not\" the RU has learned that it may be important for determining the overall sentiment of the input-text, so it needs to be stored in the memory-state of the RU, which can be used later when the RU sees the word \"good\" in time-step 6.\n",
        "\n",
        "Finally when the entire sequence has been processed, the RU outputs a vector of values that summarizes what it has seen in the input sequence. We then use a fully-connected layer with a Sigmoid activation to get a single value between 0.0 and 1.0 which we interpret as the sentiment either being negative (values close to 0.0) or positive (values close to 1.0).\n",
        "\n",
        "Note that for the sake of clarity, this figure doesn't show the mapping from text-words to integer-tokens and embedding-vectors, as well as the fully-connected Sigmoid layer on the output.\n",
        "\n",
        "![Unrolled network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo4K1kOU0K86"
      },
      "source": [
        "### 3-Layer Unrolled Network\n",
        "\n",
        "In this tutorial we will use a Recurrent Neural Network with 3 recurrent units (or layers) denoted RU1, RU2 and RU3 in the \"unrolled\" figure below.\n",
        "\n",
        "The first layer is much like the unrolled figure above for a single-layer RNN. First the recurrent unit RU1 has its internal state initialized to zero by Keras / TensorFlow. Then the word \"this\" is input to RU1 and it updates its internal state. Then it processes the next word \"is\", and so forth. But instead of outputting a single summary value at the end of the sequence, we use the output of RU1 for every time-step. This creates a new sequence that can then be used as input for the next recurrent unit RU2. The same process is repeated for the second layer and this creates a new output sequence which is then input to the third layer's recurrent unit RU3, whose final output is passed to a fully-connected Sigmoid layer that outputs a value between 0.0 (negative sentiment) and 1.0 (positive sentiment).\n",
        "\n",
        "Note that for the sake of clarity, the mapping of text-words to integer-tokens and embedding-vectors has been omitted from this figure.\n",
        "\n",
        "![Unrolled 3-layer network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_3layers_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGJRoX80K86"
      },
      "source": [
        "### Exploding & Vanishing Gradients\n",
        "\n",
        "In order to train the weights for the gates inside the recurrent unit, we need to minimize some loss-function which measures the difference between the actual output of the network relative to the desired output.\n",
        "\n",
        "From the \"unrolled\" figures above we see that the reccurent units are applied recursively for each word in the input sequence. This means the recurrent gate is applied once for each time-step. The gradient-signals have to flow back from the loss-function all the way to the first time the recurrent gate is used. If the gradient of the recurrent gate is multiplicative, then we essentially have an exponential function.\n",
        "\n",
        "In this tutorial we will use texts that have more than 500 words. This means the RU's gate for updating its internal memory-state is applied recursively more than 500 times. If a gradient of just 1.01 is multiplied with itself 500 times then it gives a value of about 145. If a gradient of just 0.99 is multiplied with itself 500 times then it gives a value of about 0.007. These are called exploding and vanishing gradients. The only gradients that can survive recurrent multiplication are 0 and 1.\n",
        "\n",
        "To avoid these so-called exploding and vanishing gradients, care must be made when designing the recurrent unit and its gates. That is why the actual implementation of the GRU is more complicated, because it tries to send the gradient back through the gates without this distortion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8mNx0eB0K87"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fayxje7Q0K88"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pABsQWxS0K89"
      },
      "source": [
        "We need to import several things from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rBvHudGh0K89"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv1bmFch0K8-"
      },
      "source": [
        "This was developed using Python 3.6 (Anaconda) and package versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "J4au_1ZI0K8-",
        "outputId": "f0071ae3-a5b9-4273-a881-23fe5c87e1f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PLUUBvW00K9A",
        "outputId": "ffcd7d1b-e99d-4cdd-b58b-7738c7c50b8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I_EXSNW0K9A"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "We will use a data-set consisting of 50000 reviews of movies from IMDB. Keras has a built-in function for downloading a similar data-set (but apparently half the size). However, Keras' version has already converted the text in the data-set to integer-tokens, which is a crucial part of working with natural languages that will also be demonstrated in this tutorial, so we download the actual text-data.\n",
        "\n",
        "NOTE: The data-set is 84 MB and will be downloaded automatically."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omg3kT9--FRf",
        "outputId": "737ffd67-bc43-420d-b643-282a97fa4902"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5N66BbSANTc",
        "outputId": "4248d856-636c-4f38-9b75-b14fdb2635c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 62% 16.0M/25.7M [00:00<00:00, 168MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 199MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JgCg5Yy40K9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935a14d2-b362-4ac4-d207-4edaa5b82209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ],
      "source": [
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8ZBc6AX0K9B"
      },
      "source": [
        "Change this if you want the files saved in another directory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "9l5GBJmJAr9g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib2XTXcN0K9C"
      },
      "source": [
        "Load the training- and test-sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb = pd.read_csv('IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "11a9VcW7AtsD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['sentiment'] = imdb['sentiment'].apply(lambda x : float(x == 'positive'))"
      ],
      "metadata": {
        "id": "RAJMcZKsA1Ja"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PjhUDb4RBZLD",
        "outputId": "fb25562a-0026-4275-bb6d-c001404b9b30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  sentiment\n",
              "0      One of the other reviewers has mentioned that ...        1.0\n",
              "1      A wonderful little production. <br /><br />The...        1.0\n",
              "2      I thought this was a wonderful way to spend ti...        1.0\n",
              "3      Basically there's a family where a little boy ...        0.0\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...        1.0\n",
              "...                                                  ...        ...\n",
              "49995  I thought this movie did a down right good job...        1.0\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...        0.0\n",
              "49997  I am a Catholic taught in parochial elementary...        0.0\n",
              "49998  I'm going to have to disagree with the previou...        0.0\n",
              "49999  No one expects the Star Trek movies to be high...        0.0\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9499240d-3070-4722-8316-ed5517c992a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9499240d-3070-4722-8316-ed5517c992a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9499240d-3070-4722-8316-ed5517c992a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9499240d-3070-4722-8316-ed5517c992a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3702050a-5747-41ec-b81b-d8888a1a0f32\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3702050a-5747-41ec-b81b-d8888a1a0f32')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3702050a-5747-41ec-b81b-d8888a1a0f32 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "imdb",
              "summary": "{\n  \"name\": \"imdb\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5000050000750013,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "giVK4JUt0K9C"
      },
      "outputs": [],
      "source": [
        "data_train = imdb[:25000]\n",
        "data_test = imdb[25000:]\n",
        "\n",
        "x_train_text = data_train['review'].tolist()\n",
        "y_train = data_train['sentiment'].values\n",
        "\n",
        "x_test_text = data_test['review'].tolist()\n",
        "y_test = data_test['sentiment'].values\n",
        "\n",
        "# x_train_text, y_train = imdb.load_data(train=True)\n",
        "# x_test_text, y_test = imdb.load_data(train=False)\n",
        "\n",
        "# # Convert to numpy arrays.\n",
        "# y_train = np.array(y_train)\n",
        "# y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FRZdqE_20K9C",
        "outputId": "56fc8ad5-e690-4683-dc7f-7ccc7437baa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-set size:  25000\n",
            "Test-set size:   25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Train-set size: \", len(x_train_text))\n",
        "print(\"Test-set size:  \", len(x_test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WHKjJL40K9D"
      },
      "source": [
        "Combine into one data-set for some uses below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1afxGpzE0K9D"
      },
      "outputs": [],
      "source": [
        "data_text = x_train_text + x_test_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUU7XkT90K9E"
      },
      "source": [
        "Print an example from the training-set to see that the data looks correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hKD7ZuTJ0K9E",
        "outputId": "ee6f4870-6fde-42b9-b8ad-f8e825a6f511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "x_train_text[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKvrRnT10K9E"
      },
      "source": [
        "The true \"class\" is a sentiment of the movie-review. It is a value of 0.0 for a negative sentiment and 1.0 for a positive sentiment. In this case the review is positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CAcMfC5m0K9E",
        "outputId": "59adc624-36ec-44c3-b078-6eec633ec1f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "y_train[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrX_y8E70K9F"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "A neural network cannot work directly on text-strings so we must convert it somehow. There are two steps in this conversion, the first step is called the \"tokenizer\" which converts words to integers and is done on the data-set before it is input to the neural network. The second step is an integrated part of the neural network itself and is called the \"embedding\"-layer, which is described further below.\n",
        "\n",
        "We may instruct the tokenizer to only use e.g. the 10000 most popular words from the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "H6zwA6jd0K9F"
      },
      "outputs": [],
      "source": [
        "num_words = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "scrolled": true,
        "id": "7TNhznnH0K9F"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOR2Dp890K9G"
      },
      "source": [
        "The tokenizer can then be \"fitted\" to the data-set. This scans through all the text and strips it from unwanted characters such as punctuation, and also converts it to lower-case characters. The tokenizer then builds a vocabulary of all unique words along with various data-structures for accessing the data.\n",
        "\n",
        "Note that we fit the tokenizer on the entire data-set so it gathers words from both the training- and test-data. This is OK as we are merely building a vocabulary and want it to be as complete as possible. The actual neural network will of course only be trained on the training-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pJhgB-X40K9G",
        "outputId": "3a0c682c-0bbb-4ad1-bfdc-9e9f784ee3cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.1 s, sys: 79.9 ms, total: 12.2 s\n",
            "Wall time: 12.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "tokenizer.fit_on_texts(data_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d7N06680K9G"
      },
      "source": [
        "If you want to use the entire vocabulary then set `num_words=None` above, and then it will automatically be set to the vocabulary-size here. (This is because of Keras' somewhat awkward implementation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "V0KzHVyl0K9H"
      },
      "outputs": [],
      "source": [
        "if num_words is None:\n",
        "    num_words = len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAmiEr4b0K9H"
      },
      "source": [
        "We can then inspect the vocabulary that has been gathered by the tokenizer. This is ordered by the number of occurrences of the words in the data-set. These integer-numbers are called word indices or \"tokens\" because they uniquely identify each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GgPzvy4n0K9H",
        "outputId": "104bea41-3100-4868-cb68-ee88a8d6d1f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'is': 6,\n",
              " 'br': 7,\n",
              " 'in': 8,\n",
              " 'it': 9,\n",
              " 'i': 10,\n",
              " 'this': 11,\n",
              " 'that': 12,\n",
              " 'was': 13,\n",
              " 'as': 14,\n",
              " 'for': 15,\n",
              " 'with': 16,\n",
              " 'movie': 17,\n",
              " 'but': 18,\n",
              " 'film': 19,\n",
              " 'on': 20,\n",
              " 'not': 21,\n",
              " 'you': 22,\n",
              " 'are': 23,\n",
              " 'his': 24,\n",
              " 'have': 25,\n",
              " 'be': 26,\n",
              " 'one': 27,\n",
              " 'he': 28,\n",
              " 'all': 29,\n",
              " 'at': 30,\n",
              " 'by': 31,\n",
              " 'an': 32,\n",
              " 'they': 33,\n",
              " 'so': 34,\n",
              " 'who': 35,\n",
              " 'from': 36,\n",
              " 'like': 37,\n",
              " 'or': 38,\n",
              " 'just': 39,\n",
              " 'her': 40,\n",
              " 'out': 41,\n",
              " 'about': 42,\n",
              " 'if': 43,\n",
              " \"it's\": 44,\n",
              " 'has': 45,\n",
              " 'there': 46,\n",
              " 'some': 47,\n",
              " 'what': 48,\n",
              " 'good': 49,\n",
              " 'when': 50,\n",
              " 'more': 51,\n",
              " 'very': 52,\n",
              " 'up': 53,\n",
              " 'no': 54,\n",
              " 'time': 55,\n",
              " 'my': 56,\n",
              " 'even': 57,\n",
              " 'would': 58,\n",
              " 'she': 59,\n",
              " 'which': 60,\n",
              " 'only': 61,\n",
              " 'really': 62,\n",
              " 'see': 63,\n",
              " 'story': 64,\n",
              " 'their': 65,\n",
              " 'had': 66,\n",
              " 'can': 67,\n",
              " 'me': 68,\n",
              " 'well': 69,\n",
              " 'were': 70,\n",
              " 'than': 71,\n",
              " 'much': 72,\n",
              " 'we': 73,\n",
              " 'bad': 74,\n",
              " 'been': 75,\n",
              " 'get': 76,\n",
              " 'do': 77,\n",
              " 'great': 78,\n",
              " 'other': 79,\n",
              " 'will': 80,\n",
              " 'also': 81,\n",
              " 'into': 82,\n",
              " 'people': 83,\n",
              " 'because': 84,\n",
              " 'how': 85,\n",
              " 'first': 86,\n",
              " 'him': 87,\n",
              " 'most': 88,\n",
              " \"don't\": 89,\n",
              " 'made': 90,\n",
              " 'then': 91,\n",
              " 'its': 92,\n",
              " 'them': 93,\n",
              " 'make': 94,\n",
              " 'way': 95,\n",
              " 'too': 96,\n",
              " 'movies': 97,\n",
              " 'could': 98,\n",
              " 'any': 99,\n",
              " 'after': 100,\n",
              " 'think': 101,\n",
              " 'characters': 102,\n",
              " 'watch': 103,\n",
              " 'films': 104,\n",
              " 'two': 105,\n",
              " 'many': 106,\n",
              " 'seen': 107,\n",
              " 'character': 108,\n",
              " 'being': 109,\n",
              " 'never': 110,\n",
              " 'plot': 111,\n",
              " 'love': 112,\n",
              " 'acting': 113,\n",
              " 'life': 114,\n",
              " 'did': 115,\n",
              " 'best': 116,\n",
              " 'where': 117,\n",
              " 'know': 118,\n",
              " 'show': 119,\n",
              " 'little': 120,\n",
              " 'over': 121,\n",
              " 'off': 122,\n",
              " 'ever': 123,\n",
              " 'does': 124,\n",
              " 'your': 125,\n",
              " 'better': 126,\n",
              " 'end': 127,\n",
              " 'man': 128,\n",
              " 'scene': 129,\n",
              " 'still': 130,\n",
              " 'say': 131,\n",
              " 'these': 132,\n",
              " 'here': 133,\n",
              " 'scenes': 134,\n",
              " 'why': 135,\n",
              " 'while': 136,\n",
              " 'something': 137,\n",
              " 'such': 138,\n",
              " 'go': 139,\n",
              " 'through': 140,\n",
              " 'back': 141,\n",
              " 'should': 142,\n",
              " 'those': 143,\n",
              " 'real': 144,\n",
              " \"i'm\": 145,\n",
              " 'now': 146,\n",
              " 'watching': 147,\n",
              " 'thing': 148,\n",
              " \"doesn't\": 149,\n",
              " 'actors': 150,\n",
              " 'though': 151,\n",
              " 'funny': 152,\n",
              " 'years': 153,\n",
              " \"didn't\": 154,\n",
              " 'old': 155,\n",
              " '10': 156,\n",
              " 'another': 157,\n",
              " 'work': 158,\n",
              " 'before': 159,\n",
              " 'actually': 160,\n",
              " 'nothing': 161,\n",
              " 'makes': 162,\n",
              " 'look': 163,\n",
              " 'director': 164,\n",
              " 'find': 165,\n",
              " 'going': 166,\n",
              " 'same': 167,\n",
              " 'new': 168,\n",
              " 'lot': 169,\n",
              " 'every': 170,\n",
              " 'few': 171,\n",
              " 'again': 172,\n",
              " 'part': 173,\n",
              " 'cast': 174,\n",
              " 'down': 175,\n",
              " 'us': 176,\n",
              " 'things': 177,\n",
              " 'want': 178,\n",
              " 'quite': 179,\n",
              " 'pretty': 180,\n",
              " 'world': 181,\n",
              " 'horror': 182,\n",
              " 'around': 183,\n",
              " 'seems': 184,\n",
              " \"can't\": 185,\n",
              " 'young': 186,\n",
              " 'take': 187,\n",
              " 'however': 188,\n",
              " 'got': 189,\n",
              " 'thought': 190,\n",
              " 'big': 191,\n",
              " 'fact': 192,\n",
              " 'enough': 193,\n",
              " 'long': 194,\n",
              " 'both': 195,\n",
              " \"that's\": 196,\n",
              " 'give': 197,\n",
              " \"i've\": 198,\n",
              " 'own': 199,\n",
              " 'may': 200,\n",
              " 'between': 201,\n",
              " 'comedy': 202,\n",
              " 'right': 203,\n",
              " 'series': 204,\n",
              " 'action': 205,\n",
              " 'must': 206,\n",
              " 'music': 207,\n",
              " 'without': 208,\n",
              " 'times': 209,\n",
              " 'saw': 210,\n",
              " 'always': 211,\n",
              " 'original': 212,\n",
              " \"isn't\": 213,\n",
              " 'role': 214,\n",
              " 'come': 215,\n",
              " 'almost': 216,\n",
              " 'gets': 217,\n",
              " 'interesting': 218,\n",
              " 'guy': 219,\n",
              " 'point': 220,\n",
              " 'done': 221,\n",
              " \"there's\": 222,\n",
              " 'whole': 223,\n",
              " 'least': 224,\n",
              " 'far': 225,\n",
              " 'bit': 226,\n",
              " 'script': 227,\n",
              " 'minutes': 228,\n",
              " 'feel': 229,\n",
              " '2': 230,\n",
              " 'anything': 231,\n",
              " 'making': 232,\n",
              " 'might': 233,\n",
              " 'since': 234,\n",
              " 'am': 235,\n",
              " 'family': 236,\n",
              " \"he's\": 237,\n",
              " 'last': 238,\n",
              " 'probably': 239,\n",
              " 'tv': 240,\n",
              " 'performance': 241,\n",
              " 'kind': 242,\n",
              " 'away': 243,\n",
              " 'yet': 244,\n",
              " 'fun': 245,\n",
              " 'worst': 246,\n",
              " 'sure': 247,\n",
              " 'rather': 248,\n",
              " 'hard': 249,\n",
              " 'anyone': 250,\n",
              " 'girl': 251,\n",
              " 'each': 252,\n",
              " 'played': 253,\n",
              " 'day': 254,\n",
              " 'found': 255,\n",
              " 'looking': 256,\n",
              " 'woman': 257,\n",
              " 'screen': 258,\n",
              " 'although': 259,\n",
              " 'our': 260,\n",
              " 'especially': 261,\n",
              " 'believe': 262,\n",
              " 'having': 263,\n",
              " 'trying': 264,\n",
              " 'course': 265,\n",
              " 'dvd': 266,\n",
              " 'everything': 267,\n",
              " 'set': 268,\n",
              " 'goes': 269,\n",
              " 'comes': 270,\n",
              " 'put': 271,\n",
              " 'ending': 272,\n",
              " 'maybe': 273,\n",
              " 'place': 274,\n",
              " 'book': 275,\n",
              " 'shows': 276,\n",
              " 'three': 277,\n",
              " 'worth': 278,\n",
              " 'different': 279,\n",
              " 'main': 280,\n",
              " 'once': 281,\n",
              " 'sense': 282,\n",
              " 'american': 283,\n",
              " 'reason': 284,\n",
              " 'looks': 285,\n",
              " 'effects': 286,\n",
              " 'watched': 287,\n",
              " 'play': 288,\n",
              " 'true': 289,\n",
              " 'money': 290,\n",
              " 'actor': 291,\n",
              " \"wasn't\": 292,\n",
              " 'job': 293,\n",
              " 'together': 294,\n",
              " 'war': 295,\n",
              " 'someone': 296,\n",
              " 'plays': 297,\n",
              " 'instead': 298,\n",
              " 'high': 299,\n",
              " 'during': 300,\n",
              " 'year': 301,\n",
              " 'said': 302,\n",
              " 'half': 303,\n",
              " 'everyone': 304,\n",
              " 'later': 305,\n",
              " 'takes': 306,\n",
              " '1': 307,\n",
              " 'seem': 308,\n",
              " 'audience': 309,\n",
              " 'special': 310,\n",
              " 'beautiful': 311,\n",
              " 'left': 312,\n",
              " 'himself': 313,\n",
              " 'seeing': 314,\n",
              " 'john': 315,\n",
              " 'night': 316,\n",
              " 'black': 317,\n",
              " 'version': 318,\n",
              " 'shot': 319,\n",
              " 'excellent': 320,\n",
              " 'idea': 321,\n",
              " 'house': 322,\n",
              " 'mind': 323,\n",
              " 'star': 324,\n",
              " 'wife': 325,\n",
              " 'fan': 326,\n",
              " 'death': 327,\n",
              " 'used': 328,\n",
              " 'else': 329,\n",
              " 'simply': 330,\n",
              " 'nice': 331,\n",
              " 'budget': 332,\n",
              " 'poor': 333,\n",
              " 'short': 334,\n",
              " 'completely': 335,\n",
              " 'second': 336,\n",
              " \"you're\": 337,\n",
              " '3': 338,\n",
              " 'read': 339,\n",
              " 'less': 340,\n",
              " 'along': 341,\n",
              " 'top': 342,\n",
              " 'help': 343,\n",
              " 'home': 344,\n",
              " 'men': 345,\n",
              " 'either': 346,\n",
              " 'line': 347,\n",
              " 'boring': 348,\n",
              " 'dead': 349,\n",
              " 'friends': 350,\n",
              " 'kids': 351,\n",
              " 'try': 352,\n",
              " 'production': 353,\n",
              " 'enjoy': 354,\n",
              " 'camera': 355,\n",
              " 'use': 356,\n",
              " 'wrong': 357,\n",
              " 'given': 358,\n",
              " 'low': 359,\n",
              " 'classic': 360,\n",
              " 'father': 361,\n",
              " 'need': 362,\n",
              " 'full': 363,\n",
              " 'stupid': 364,\n",
              " 'next': 365,\n",
              " 'until': 366,\n",
              " 'performances': 367,\n",
              " 'school': 368,\n",
              " 'hollywood': 369,\n",
              " 'rest': 370,\n",
              " 'truly': 371,\n",
              " 'awful': 372,\n",
              " 'video': 373,\n",
              " 'couple': 374,\n",
              " 'start': 375,\n",
              " 'sex': 376,\n",
              " 'recommend': 377,\n",
              " 'women': 378,\n",
              " 'let': 379,\n",
              " 'tell': 380,\n",
              " 'terrible': 381,\n",
              " 'remember': 382,\n",
              " 'mean': 383,\n",
              " 'came': 384,\n",
              " 'getting': 385,\n",
              " 'understand': 386,\n",
              " 'perhaps': 387,\n",
              " 'moments': 388,\n",
              " 'name': 389,\n",
              " 'keep': 390,\n",
              " 'face': 391,\n",
              " 'itself': 392,\n",
              " 'wonderful': 393,\n",
              " 'playing': 394,\n",
              " 'human': 395,\n",
              " 'style': 396,\n",
              " 'small': 397,\n",
              " 'episode': 398,\n",
              " 'perfect': 399,\n",
              " 'others': 400,\n",
              " 'person': 401,\n",
              " 'doing': 402,\n",
              " 'often': 403,\n",
              " 'early': 404,\n",
              " 'stars': 405,\n",
              " 'definitely': 406,\n",
              " 'written': 407,\n",
              " 'head': 408,\n",
              " 'lines': 409,\n",
              " 'dialogue': 410,\n",
              " 'gives': 411,\n",
              " 'piece': 412,\n",
              " \"couldn't\": 413,\n",
              " 'went': 414,\n",
              " 'finally': 415,\n",
              " 'mother': 416,\n",
              " 'case': 417,\n",
              " 'title': 418,\n",
              " 'absolutely': 419,\n",
              " 'boy': 420,\n",
              " 'live': 421,\n",
              " 'yes': 422,\n",
              " 'laugh': 423,\n",
              " 'certainly': 424,\n",
              " 'liked': 425,\n",
              " 'become': 426,\n",
              " 'entertaining': 427,\n",
              " 'worse': 428,\n",
              " 'oh': 429,\n",
              " 'sort': 430,\n",
              " 'loved': 431,\n",
              " 'lost': 432,\n",
              " 'called': 433,\n",
              " 'hope': 434,\n",
              " 'picture': 435,\n",
              " 'felt': 436,\n",
              " 'overall': 437,\n",
              " 'entire': 438,\n",
              " 'mr': 439,\n",
              " 'several': 440,\n",
              " 'based': 441,\n",
              " 'supposed': 442,\n",
              " 'cinema': 443,\n",
              " 'friend': 444,\n",
              " 'guys': 445,\n",
              " 'sound': 446,\n",
              " '5': 447,\n",
              " 'problem': 448,\n",
              " 'drama': 449,\n",
              " 'against': 450,\n",
              " 'waste': 451,\n",
              " 'white': 452,\n",
              " 'beginning': 453,\n",
              " '4': 454,\n",
              " 'fans': 455,\n",
              " 'totally': 456,\n",
              " 'dark': 457,\n",
              " 'care': 458,\n",
              " 'direction': 459,\n",
              " 'humor': 460,\n",
              " 'wanted': 461,\n",
              " \"she's\": 462,\n",
              " 'seemed': 463,\n",
              " 'under': 464,\n",
              " 'game': 465,\n",
              " 'children': 466,\n",
              " 'despite': 467,\n",
              " 'lives': 468,\n",
              " 'lead': 469,\n",
              " 'guess': 470,\n",
              " 'example': 471,\n",
              " 'already': 472,\n",
              " 'final': 473,\n",
              " \"you'll\": 474,\n",
              " 'throughout': 475,\n",
              " 'evil': 476,\n",
              " 'turn': 477,\n",
              " 'becomes': 478,\n",
              " 'unfortunately': 479,\n",
              " 'able': 480,\n",
              " 'quality': 481,\n",
              " \"i'd\": 482,\n",
              " 'days': 483,\n",
              " 'history': 484,\n",
              " 'fine': 485,\n",
              " 'side': 486,\n",
              " 'wants': 487,\n",
              " 'horrible': 488,\n",
              " 'heart': 489,\n",
              " 'writing': 490,\n",
              " 'amazing': 491,\n",
              " 'b': 492,\n",
              " 'flick': 493,\n",
              " 'killer': 494,\n",
              " 'run': 495,\n",
              " 'son': 496,\n",
              " '\\x96': 497,\n",
              " 'michael': 498,\n",
              " 'works': 499,\n",
              " 'close': 500,\n",
              " \"they're\": 501,\n",
              " 'act': 502,\n",
              " 'art': 503,\n",
              " 'kill': 504,\n",
              " 'matter': 505,\n",
              " 'etc': 506,\n",
              " 'tries': 507,\n",
              " \"won't\": 508,\n",
              " 'past': 509,\n",
              " 'town': 510,\n",
              " 'enjoyed': 511,\n",
              " 'turns': 512,\n",
              " 'brilliant': 513,\n",
              " 'gave': 514,\n",
              " 'behind': 515,\n",
              " 'parts': 516,\n",
              " 'stuff': 517,\n",
              " 'genre': 518,\n",
              " 'eyes': 519,\n",
              " 'car': 520,\n",
              " 'favorite': 521,\n",
              " 'directed': 522,\n",
              " 'late': 523,\n",
              " 'hand': 524,\n",
              " 'expect': 525,\n",
              " 'soon': 526,\n",
              " 'hour': 527,\n",
              " 'obviously': 528,\n",
              " 'themselves': 529,\n",
              " 'sometimes': 530,\n",
              " 'killed': 531,\n",
              " 'thinking': 532,\n",
              " 'actress': 533,\n",
              " 'child': 534,\n",
              " 'girls': 535,\n",
              " 'viewer': 536,\n",
              " 'starts': 537,\n",
              " 'city': 538,\n",
              " 'myself': 539,\n",
              " 'decent': 540,\n",
              " 'highly': 541,\n",
              " 'stop': 542,\n",
              " 'type': 543,\n",
              " 'self': 544,\n",
              " 'god': 545,\n",
              " 'says': 546,\n",
              " 'group': 547,\n",
              " 'anyway': 548,\n",
              " 'voice': 549,\n",
              " 'took': 550,\n",
              " 'known': 551,\n",
              " 'blood': 552,\n",
              " 'kid': 553,\n",
              " 'heard': 554,\n",
              " 'happens': 555,\n",
              " 'except': 556,\n",
              " 'fight': 557,\n",
              " 'feeling': 558,\n",
              " 'experience': 559,\n",
              " 'coming': 560,\n",
              " 'slow': 561,\n",
              " 'daughter': 562,\n",
              " 'writer': 563,\n",
              " 'stories': 564,\n",
              " 'moment': 565,\n",
              " 'leave': 566,\n",
              " 'told': 567,\n",
              " 'extremely': 568,\n",
              " 'score': 569,\n",
              " 'violence': 570,\n",
              " 'involved': 571,\n",
              " 'police': 572,\n",
              " 'strong': 573,\n",
              " 'lack': 574,\n",
              " 'chance': 575,\n",
              " 'cannot': 576,\n",
              " 'hit': 577,\n",
              " 'roles': 578,\n",
              " 'hilarious': 579,\n",
              " 's': 580,\n",
              " 'wonder': 581,\n",
              " 'happen': 582,\n",
              " 'particularly': 583,\n",
              " 'ok': 584,\n",
              " 'including': 585,\n",
              " 'save': 586,\n",
              " 'living': 587,\n",
              " 'looked': 588,\n",
              " \"wouldn't\": 589,\n",
              " 'crap': 590,\n",
              " 'please': 591,\n",
              " 'simple': 592,\n",
              " 'murder': 593,\n",
              " 'cool': 594,\n",
              " 'obvious': 595,\n",
              " 'happened': 596,\n",
              " 'complete': 597,\n",
              " 'cut': 598,\n",
              " 'serious': 599,\n",
              " 'age': 600,\n",
              " 'gore': 601,\n",
              " 'attempt': 602,\n",
              " 'hell': 603,\n",
              " 'ago': 604,\n",
              " 'song': 605,\n",
              " 'shown': 606,\n",
              " 'taken': 607,\n",
              " 'english': 608,\n",
              " 'james': 609,\n",
              " 'robert': 610,\n",
              " 'david': 611,\n",
              " 'seriously': 612,\n",
              " 'released': 613,\n",
              " 'reality': 614,\n",
              " 'opening': 615,\n",
              " 'jokes': 616,\n",
              " 'interest': 617,\n",
              " 'across': 618,\n",
              " 'none': 619,\n",
              " 'hero': 620,\n",
              " 'exactly': 621,\n",
              " 'today': 622,\n",
              " 'possible': 623,\n",
              " 'alone': 624,\n",
              " 'sad': 625,\n",
              " 'brother': 626,\n",
              " 'number': 627,\n",
              " 'career': 628,\n",
              " 'saying': 629,\n",
              " \"film's\": 630,\n",
              " 'usually': 631,\n",
              " 'hours': 632,\n",
              " 'cinematography': 633,\n",
              " 'talent': 634,\n",
              " 'view': 635,\n",
              " 'annoying': 636,\n",
              " 'running': 637,\n",
              " 'yourself': 638,\n",
              " 'relationship': 639,\n",
              " 'documentary': 640,\n",
              " 'wish': 641,\n",
              " 'order': 642,\n",
              " 'huge': 643,\n",
              " 'shots': 644,\n",
              " 'whose': 645,\n",
              " 'ridiculous': 646,\n",
              " 'taking': 647,\n",
              " 'important': 648,\n",
              " 'light': 649,\n",
              " 'body': 650,\n",
              " 'middle': 651,\n",
              " 'level': 652,\n",
              " 'ends': 653,\n",
              " 'started': 654,\n",
              " 'female': 655,\n",
              " 'call': 656,\n",
              " \"i'll\": 657,\n",
              " 'husband': 658,\n",
              " 'four': 659,\n",
              " 'power': 660,\n",
              " 'word': 661,\n",
              " 'turned': 662,\n",
              " 'major': 663,\n",
              " 'opinion': 664,\n",
              " 'change': 665,\n",
              " 'mostly': 666,\n",
              " 'usual': 667,\n",
              " 'scary': 668,\n",
              " 'silly': 669,\n",
              " 'rating': 670,\n",
              " 'beyond': 671,\n",
              " 'somewhat': 672,\n",
              " 'happy': 673,\n",
              " 'ones': 674,\n",
              " 'words': 675,\n",
              " 'room': 676,\n",
              " 'knows': 677,\n",
              " 'knew': 678,\n",
              " 'country': 679,\n",
              " 'disappointed': 680,\n",
              " 'talking': 681,\n",
              " 'novel': 682,\n",
              " 'apparently': 683,\n",
              " 'non': 684,\n",
              " 'strange': 685,\n",
              " 'upon': 686,\n",
              " 'attention': 687,\n",
              " 'basically': 688,\n",
              " 'single': 689,\n",
              " 'finds': 690,\n",
              " 'cheap': 691,\n",
              " 'modern': 692,\n",
              " 'due': 693,\n",
              " 'jack': 694,\n",
              " 'television': 695,\n",
              " 'musical': 696,\n",
              " 'problems': 697,\n",
              " 'miss': 698,\n",
              " 'episodes': 699,\n",
              " 'clearly': 700,\n",
              " 'local': 701,\n",
              " '7': 702,\n",
              " 'british': 703,\n",
              " 'thriller': 704,\n",
              " 'talk': 705,\n",
              " 'events': 706,\n",
              " 'sequence': 707,\n",
              " 'five': 708,\n",
              " \"aren't\": 709,\n",
              " 'class': 710,\n",
              " 'french': 711,\n",
              " 'moving': 712,\n",
              " 'ten': 713,\n",
              " 'fast': 714,\n",
              " 'earth': 715,\n",
              " 'review': 716,\n",
              " 'tells': 717,\n",
              " 'predictable': 718,\n",
              " 'songs': 719,\n",
              " 'team': 720,\n",
              " 'comic': 721,\n",
              " 'straight': 722,\n",
              " '8': 723,\n",
              " 'whether': 724,\n",
              " 'die': 725,\n",
              " 'add': 726,\n",
              " 'dialog': 727,\n",
              " 'entertainment': 728,\n",
              " 'above': 729,\n",
              " 'sets': 730,\n",
              " 'future': 731,\n",
              " 'enjoyable': 732,\n",
              " 'appears': 733,\n",
              " 'near': 734,\n",
              " 'space': 735,\n",
              " 'easily': 736,\n",
              " 'hate': 737,\n",
              " 'soundtrack': 738,\n",
              " 'bring': 739,\n",
              " 'giving': 740,\n",
              " 'lots': 741,\n",
              " 'similar': 742,\n",
              " 'romantic': 743,\n",
              " 'george': 744,\n",
              " 'supporting': 745,\n",
              " 'release': 746,\n",
              " 'mention': 747,\n",
              " 'within': 748,\n",
              " 'filmed': 749,\n",
              " 'message': 750,\n",
              " 'sequel': 751,\n",
              " 'clear': 752,\n",
              " 'falls': 753,\n",
              " \"haven't\": 754,\n",
              " 'needs': 755,\n",
              " 'dull': 756,\n",
              " 'suspense': 757,\n",
              " 'bunch': 758,\n",
              " 'eye': 759,\n",
              " 'surprised': 760,\n",
              " 'showing': 761,\n",
              " 'tried': 762,\n",
              " 'sorry': 763,\n",
              " 'certain': 764,\n",
              " 'working': 765,\n",
              " 'easy': 766,\n",
              " 'ways': 767,\n",
              " 'theme': 768,\n",
              " 'theater': 769,\n",
              " 'among': 770,\n",
              " 'named': 771,\n",
              " \"what's\": 772,\n",
              " 'storyline': 773,\n",
              " 'monster': 774,\n",
              " 'king': 775,\n",
              " 'stay': 776,\n",
              " 'effort': 777,\n",
              " 'minute': 778,\n",
              " 'fall': 779,\n",
              " 'stand': 780,\n",
              " 'gone': 781,\n",
              " 'rock': 782,\n",
              " 'using': 783,\n",
              " '9': 784,\n",
              " 'feature': 785,\n",
              " 'comments': 786,\n",
              " 'buy': 787,\n",
              " \"'\": 788,\n",
              " 'typical': 789,\n",
              " 't': 790,\n",
              " 'editing': 791,\n",
              " 'sister': 792,\n",
              " 'tale': 793,\n",
              " 'avoid': 794,\n",
              " 'dr': 795,\n",
              " 'mystery': 796,\n",
              " 'deal': 797,\n",
              " 'doubt': 798,\n",
              " 'fantastic': 799,\n",
              " 'kept': 800,\n",
              " 'nearly': 801,\n",
              " 'feels': 802,\n",
              " 'subject': 803,\n",
              " 'okay': 804,\n",
              " 'viewing': 805,\n",
              " 'elements': 806,\n",
              " 'oscar': 807,\n",
              " 'check': 808,\n",
              " 'points': 809,\n",
              " 'realistic': 810,\n",
              " 'means': 811,\n",
              " 'greatest': 812,\n",
              " 'herself': 813,\n",
              " 'parents': 814,\n",
              " 'famous': 815,\n",
              " 'imagine': 816,\n",
              " 'rent': 817,\n",
              " 'viewers': 818,\n",
              " 'richard': 819,\n",
              " 'crime': 820,\n",
              " 'form': 821,\n",
              " 'peter': 822,\n",
              " 'actual': 823,\n",
              " 'lady': 824,\n",
              " 'general': 825,\n",
              " 'dog': 826,\n",
              " 'follow': 827,\n",
              " 'believable': 828,\n",
              " 'period': 829,\n",
              " 'red': 830,\n",
              " 'brought': 831,\n",
              " 'move': 832,\n",
              " 'material': 833,\n",
              " 'forget': 834,\n",
              " 'somehow': 835,\n",
              " 'begins': 836,\n",
              " 're': 837,\n",
              " 'reviews': 838,\n",
              " 'animation': 839,\n",
              " 'paul': 840,\n",
              " \"you've\": 841,\n",
              " 'leads': 842,\n",
              " 'weak': 843,\n",
              " 'figure': 844,\n",
              " 'surprise': 845,\n",
              " 'hear': 846,\n",
              " 'sit': 847,\n",
              " 'average': 848,\n",
              " 'open': 849,\n",
              " 'sequences': 850,\n",
              " 'killing': 851,\n",
              " 'atmosphere': 852,\n",
              " 'eventually': 853,\n",
              " 'tom': 854,\n",
              " 'learn': 855,\n",
              " 'premise': 856,\n",
              " 'wait': 857,\n",
              " '20': 858,\n",
              " 'sci': 859,\n",
              " 'deep': 860,\n",
              " 'fi': 861,\n",
              " 'expected': 862,\n",
              " 'whatever': 863,\n",
              " 'indeed': 864,\n",
              " 'lame': 865,\n",
              " 'poorly': 866,\n",
              " 'particular': 867,\n",
              " 'note': 868,\n",
              " 'dance': 869,\n",
              " 'imdb': 870,\n",
              " 'shame': 871,\n",
              " 'situation': 872,\n",
              " 'third': 873,\n",
              " 'york': 874,\n",
              " 'box': 875,\n",
              " 'truth': 876,\n",
              " 'decided': 877,\n",
              " 'free': 878,\n",
              " 'hot': 879,\n",
              " \"who's\": 880,\n",
              " 'difficult': 881,\n",
              " 'needed': 882,\n",
              " 'season': 883,\n",
              " 'acted': 884,\n",
              " 'leaves': 885,\n",
              " 'unless': 886,\n",
              " 'romance': 887,\n",
              " 'emotional': 888,\n",
              " 'possibly': 889,\n",
              " 'gay': 890,\n",
              " 'sexual': 891,\n",
              " 'boys': 892,\n",
              " 'footage': 893,\n",
              " 'write': 894,\n",
              " 'western': 895,\n",
              " 'credits': 896,\n",
              " 'forced': 897,\n",
              " 'memorable': 898,\n",
              " 'doctor': 899,\n",
              " 'reading': 900,\n",
              " 'became': 901,\n",
              " 'otherwise': 902,\n",
              " 'air': 903,\n",
              " 'begin': 904,\n",
              " 'de': 905,\n",
              " 'crew': 906,\n",
              " 'question': 907,\n",
              " 'meet': 908,\n",
              " 'society': 909,\n",
              " 'male': 910,\n",
              " \"let's\": 911,\n",
              " 'meets': 912,\n",
              " 'plus': 913,\n",
              " 'cheesy': 914,\n",
              " 'hands': 915,\n",
              " 'superb': 916,\n",
              " 'screenplay': 917,\n",
              " 'beauty': 918,\n",
              " 'interested': 919,\n",
              " 'street': 920,\n",
              " 'features': 921,\n",
              " 'perfectly': 922,\n",
              " 'masterpiece': 923,\n",
              " 'whom': 924,\n",
              " 'laughs': 925,\n",
              " 'nature': 926,\n",
              " 'stage': 927,\n",
              " 'effect': 928,\n",
              " 'forward': 929,\n",
              " 'comment': 930,\n",
              " 'nor': 931,\n",
              " 'previous': 932,\n",
              " 'e': 933,\n",
              " 'badly': 934,\n",
              " 'sounds': 935,\n",
              " 'japanese': 936,\n",
              " 'weird': 937,\n",
              " 'island': 938,\n",
              " 'inside': 939,\n",
              " 'personal': 940,\n",
              " 'quickly': 941,\n",
              " 'total': 942,\n",
              " 'keeps': 943,\n",
              " 'towards': 944,\n",
              " 'america': 945,\n",
              " 'result': 946,\n",
              " 'crazy': 947,\n",
              " 'battle': 948,\n",
              " 'worked': 949,\n",
              " 'incredibly': 950,\n",
              " 'setting': 951,\n",
              " 'earlier': 952,\n",
              " 'background': 953,\n",
              " 'mess': 954,\n",
              " 'cop': 955,\n",
              " 'writers': 956,\n",
              " 'fire': 957,\n",
              " 'copy': 958,\n",
              " 'realize': 959,\n",
              " 'dumb': 960,\n",
              " 'unique': 961,\n",
              " 'powerful': 962,\n",
              " 'mark': 963,\n",
              " 'lee': 964,\n",
              " 'business': 965,\n",
              " 'rate': 966,\n",
              " 'older': 967,\n",
              " 'dramatic': 968,\n",
              " 'pay': 969,\n",
              " 'following': 970,\n",
              " 'girlfriend': 971,\n",
              " 'directors': 972,\n",
              " 'joke': 973,\n",
              " 'plenty': 974,\n",
              " 'directing': 975,\n",
              " 'various': 976,\n",
              " 'baby': 977,\n",
              " 'creepy': 978,\n",
              " 'development': 979,\n",
              " 'appear': 980,\n",
              " 'brings': 981,\n",
              " 'front': 982,\n",
              " 'dream': 983,\n",
              " 'ask': 984,\n",
              " 'water': 985,\n",
              " 'rich': 986,\n",
              " 'bill': 987,\n",
              " 'admit': 988,\n",
              " 'apart': 989,\n",
              " 'joe': 990,\n",
              " 'political': 991,\n",
              " 'fairly': 992,\n",
              " 'leading': 993,\n",
              " 'reasons': 994,\n",
              " 'spent': 995,\n",
              " 'portrayed': 996,\n",
              " 'telling': 997,\n",
              " 'cover': 998,\n",
              " 'outside': 999,\n",
              " 'fighting': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuBO0wCh0K9H"
      },
      "source": [
        "We can then use the tokenizer to convert all texts in the training-set to lists of these tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "n3-F4QI90K9S"
      },
      "outputs": [],
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVxsPzcQETqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hff0KhIK0K9S"
      },
      "source": [
        "For example, here is a text from the training-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Hj9JypPM0K9S",
        "outputId": "571867c7-38cc-4f04-f1e8-423d132a814d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "x_train_text[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e3nBz_F0K9T"
      },
      "source": [
        "This text corresponds to the following list of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qbN3EdLW0K9T",
        "outputId": "75b2a7e3-892a-4161-e7a2-e2dc89e222bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   3,  393,  120,  353,    7,    7,    1, 1385, 2977,    6,   52,\n",
              "         52,  155,   55, 2381, 1582,    2,  411,    3,    2,  530,  282,\n",
              "          4, 1847,    5,    1,  438,  412,    7,    7,    1,  150,   23,\n",
              "        568,   69, 2274,  498, 4571,   21,   61,   45,  189,   29,    1,\n",
              "         18,   28,   45,   29,    1, 2294,  175, 3336,   96,   22,   67,\n",
              "        371,   63,    1,  791, 9719,   31,    1, 1825,    5, 7366, 6594,\n",
              "         21,   61,    6,    9,   69,  278,    1,  147,   18,    9,    6,\n",
              "          3,  407,    2, 2406,  412,    3, 4339,  353,   42,   27,    4,\n",
              "          1,   78,    4,  202,    2,   24,  114,    7,    7,    1, 1847,\n",
              "         62,  270,  344,   16,    1,  120,  177,    1, 1029,    4,    1,\n",
              "       2924,   60,  248,   71,  356,    1, 2206, 3127, 1289, 1192,   91,\n",
              "       4911,    9,  297,   20,  260, 1830,    2,  260, 4592,  583,   16,\n",
              "          1,  134, 3690,    2,    2,    1,  730,  583,    4,   65, 1054,\n",
              "         16,  170, 2297,   23, 1977,   69,  221])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "np.array(x_train_tokens[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryRn_nA0K9T"
      },
      "source": [
        "We also need to convert the texts in the test-set to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GobH0DmY0K9U"
      },
      "outputs": [],
      "source": [
        "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm63dFt20K9U"
      },
      "source": [
        "## Padding and Truncating Data\n",
        "\n",
        "The Recurrent Neural Network can take sequences of arbitrary length as input, but in order to use a whole batch of data, the sequences need to have the same length. There are two ways of achieving this: (A) Either we ensure that all sequences in the entire data-set have the same length, or (B) we write a custom data-generator that ensures the sequences have the same length within each batch.\n",
        "\n",
        "Solution (A) is simpler but if we use the length of the longest sequence in the data-set, then we are wasting a lot of memory. This is particularly important for larger data-sets.\n",
        "\n",
        "So in order to make a compromise, we will use a sequence-length that covers most sequences in the data-set, and we will then truncate longer sequences and pad shorter sequences.\n",
        "\n",
        "First we count the number of tokens in all the sequences in the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_6A-GBpC0K9U"
      },
      "outputs": [],
      "source": [
        "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
        "num_tokens = np.array(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEZoSoXK0K9U"
      },
      "source": [
        "The average number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "E-e_WHqw0K9V",
        "outputId": "1c58864a-9ac4-4d0d-df06-2747bef69df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221.27716"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "np.mean(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ODJF1Ls0K9V"
      },
      "source": [
        "The maximum number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OUBQwXGT0K9V",
        "outputId": "5b1ef91e-1a1d-4934-f2a6-2d6fe54c86ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2209"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "np.max(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjlWemgz0K9V"
      },
      "source": [
        "The max number of tokens we will allow is set to the average plus 2 standard deviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ydfvqvQX0K9W",
        "outputId": "d3843394-f363-466e-ef42-00d2b0964d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXhOPwSb0K9W"
      },
      "source": [
        "This covers about 95% of the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uROPm_qE0K9X",
        "outputId": "040b2bf6-15f0-4497-d7f6-32138531259c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9453"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVC0hQUs0K9X"
      },
      "source": [
        "When padding or truncating the sequences that have a different length, we need to determine if we want to do this padding or truncating 'pre' or 'post'. If a sequence is truncated, it means that a part of the sequence is simply thrown away. If a sequence is padded, it means that zeros are added to the sequence.\n",
        "\n",
        "So the choice of 'pre' or 'post' can be important because it determines whether we throw away the first or last part of a sequence when truncating, and it determines whether we add zeros to the beginning or end of the sequence when padding. This may confuse the Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EpcJHVm70K9Y"
      },
      "outputs": [],
      "source": [
        "pad = 'pre'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "E_2S34Ld0K9Y"
      },
      "outputs": [],
      "source": [
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
        "                            padding=pad, truncating=pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6VXIBon00K9Z"
      },
      "outputs": [],
      "source": [
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
        "                           padding=pad, truncating=pad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_pad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOq_Oj_VJAOx",
        "outputId": "0159800e-303d-479a-a791-25c3f27d1371"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhEAzm_w0K9a"
      },
      "source": [
        "We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "722SNb-40K9a",
        "outputId": "da6647fb-0719-4ec0-f3f7-dd0a3e3b64e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYzeNKlV0K9b"
      },
      "source": [
        "The matrix for the test-set has the same shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UD02hVXJ0K9b",
        "outputId": "93e48588-d7a8-4a0c-ec82-583876e29b21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6qXb0vt0K9c"
      },
      "source": [
        "For example, we had the following sequence of tokens above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfGtpLpU0K9c",
        "outputId": "3394b89f-3ad8-45bf-d7cd-042744d81a2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1037,    5,    1,  432,  181,   13,  748,  141,    5,  141,   16,\n",
              "          1, 7418,  318,    4,    1,  432,  181,    7,    7,    8,   11,\n",
              "        751,    1,  167,  707,   83,  469,   31, 1037,    5,    1,  117,\n",
              "          3,  547,   45,  654,   15, 3336,   60,    6, 3746,    5, 2222,\n",
              "          1, 1379,   45,  137,    5,   77,   16,   11,   33, 1894,    5,\n",
              "       4399,    1,    2,    1,    6, 2011,   72,    5,    1, 3029,    4,\n",
              "          1, 5836,    7,    7,   37,    8,    1,  432,  181,   48,  171,\n",
              "       3847,   73,   63,   23,   90,    4, 4118,    2,  132, 1469,    3,\n",
              "        789, 4019,    2,    7,    7,  315, 4180,    2,  611, 2991,   65,\n",
              "        578,   14,    2,    2,  277,    4,    1,   79,  150,   23,   81,\n",
              "        141,    7,    7,  467,  899,  439,   74,  838,    4,   11,    2,\n",
              "        143,  691,  256, 4118, 3847,   10,  511, 1037,    5,    1,  432,\n",
              "        181,    7,    7,  670,  338,  405,   41,    4,  447])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(x_train_tokens[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a20gZwiE0K9c"
      },
      "source": [
        "This has simply been padded to create the following sequence. Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "MqfhNY5D0K9d",
        "outputId": "1a4bd43d-aee4-4228-cf5c-9e5e85e2f9d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    3,  393,\n",
              "        120,  353,    7,    7,    1, 1385, 2977,    6,   52,   52,  155,\n",
              "         55, 2381, 1582,    2,  411,    3,    2,  530,  282,    4, 1847,\n",
              "          5,    1,  438,  412,    7,    7,    1,  150,   23,  568,   69,\n",
              "       2274,  498, 4571,   21,   61,   45,  189,   29,    1,   18,   28,\n",
              "         45,   29,    1, 2294,  175, 3336,   96,   22,   67,  371,   63,\n",
              "          1,  791, 9719,   31,    1, 1825,    5, 7366, 6594,   21,   61,\n",
              "          6,    9,   69,  278,    1,  147,   18,    9,    6,    3,  407,\n",
              "          2, 2406,  412,    3, 4339,  353,   42,   27,    4,    1,   78,\n",
              "          4,  202,    2,   24,  114,    7,    7,    1, 1847,   62,  270,\n",
              "        344,   16,    1,  120,  177,    1, 1029,    4,    1, 2924,   60,\n",
              "        248,   71,  356,    1, 2206, 3127, 1289, 1192,   91, 4911,    9,\n",
              "        297,   20,  260, 1830,    2,  260, 4592,  583,   16,    1,  134,\n",
              "       3690,    2,    2,    1,  730,  583,    4,   65, 1054,   16,  170,\n",
              "       2297,   23, 1977,   69,  221], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "x_train_pad[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNm_KzoH0K9d"
      },
      "source": [
        "## Tokenizer Inverse Map\n",
        "\n",
        "For some strange reason, the Keras implementation of a tokenizer does not seem to have the inverse mapping from integer-tokens back to words, which is needed to reconstruct text-strings from lists of tokens. So we make that mapping here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "AymNtHQx0K9d"
      },
      "outputs": [],
      "source": [
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_map"
      ],
      "metadata": {
        "id": "f9Zp-5fjLyNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNhnczIG0K9d"
      },
      "source": [
        "Helper-function for converting a list of tokens back to a string of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "KW7-JECe0K9e"
      },
      "outputs": [],
      "source": [
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "\n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCK898lR0K9e"
      },
      "source": [
        "For example, this is the original text from the data-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "scrolled": true,
        "id": "pShnRM430K9e",
        "outputId": "a71ec458-5e45-4adb-cda2-9f4c9b798748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "x_train_text[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ltyiEe0K9e"
      },
      "source": [
        "We can recreate this text except for punctuation and other symbols, by converting the list of tokens back to words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "nLxLfTbi0K9e",
        "outputId": "ff0fb879-51ff-44e1-dd5f-08d4a1ff1856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a wonderful little production br br the filming technique is very very old time bbc fashion and gives a and sometimes sense of realism to the entire piece br br the actors are extremely well chosen michael sheen not only has got all the but he has all the voices down pat too you can truly see the editing guided by the references to diary entries not only is it well worth the watching but it is a written and performed piece a masterful production about one of the great of comedy and his life br br the realism really comes home with the little things the fantasy of the guard which rather than use the traditional techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning and and the sets particularly of their flat with every surface are terribly well done'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "tokens_to_string(x_train_tokens[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqoConZU0K9f"
      },
      "source": [
        "## Create the Recurrent Neural Network\n",
        "\n",
        "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "uKvVmr6x0K9f"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTaKBDQ60K9f"
      },
      "source": [
        "The first layer in the RNN is a so-called Embedding-layer which converts each integer-token into a vector of values. This is necessary because the integer-tokens may take on values between 0 and 10000 for a vocabulary of 10000 words. The RNN cannot work on values in such a wide range. The embedding-layer is trained as a part of the RNN and will learn to map words with similar semantic meanings to similar embedding-vectors, as will be shown further below.\n",
        "\n",
        "First we define the size of the embedding-vector for each integer-token. In this case we have set it to 8, so that each integer-token will be converted to a vector of length 8. The values of the embedding-vector will generally fall roughly between -1.0 and 1.0, although they may exceed these values somewhat.\n",
        "\n",
        "The size of the embedding-vector is typically selected between 100-300, but it seems to work reasonably well with small values for Sentiment Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "HVi0PbKB0K9f"
      },
      "outputs": [],
      "source": [
        "embedding_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVrIrM6b0K9g"
      },
      "source": [
        "The embedding-layer also needs to know the number of words in the vocabulary (`num_words`) and the length of the padded token-sequences (`max_tokens`). We also give this layer a name because we need to retrieve its weights further below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh38Ajq4Mwbp",
        "outputId": "068058ea-a924-4c4c-9869-47b7cf27ad38"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oUSOLsFG0K9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b70473-fde7-4372-9e11-9c426bc12463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens,\n",
        "                    name='layer_embedding'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js-rqOe80K9g"
      },
      "source": [
        "We can now add the first Gated Recurrent Unit (GRU) to the network. This will have 16 outputs. Because we will add a second GRU after this one, we need to return sequences of data because the next GRU expects sequences as its input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Zk44ywBn0K9g"
      },
      "outputs": [],
      "source": [
        "model.add(GRU(units=16, return_sequences=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcFa4NIR0K9g"
      },
      "source": [
        "This adds the second GRU with 8 output units. This will be followed by another GRU so it must also return sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "sbqcLSrk0K9h"
      },
      "outputs": [],
      "source": [
        "model.add(GRU(units=8, return_sequences=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzsCklmq0K9h"
      },
      "source": [
        "This adds the third and final GRU with 4 output units. This will be followed by a dense-layer, so it should only give the final output of the GRU and not a whole sequence of outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "pd8Avb9v0K9h"
      },
      "outputs": [],
      "source": [
        "model.add(GRU(units=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F_fbsdi0K9h"
      },
      "source": [
        "Add a fully-connected / dense layer which computes a value between 0.0 and 1.0 that will be used as the classification output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "FDq62g5y0K9l"
      },
      "outputs": [],
      "source": [
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0AnEcPd0K9l"
      },
      "source": [
        "Use the Adam optimizer with the given learning-rate."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "tsuvCQSxNbZs",
        "outputId": "f66f3606-e27e-4d73-bb46-5dbc329c2079"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ layer_embedding (\u001b[38;5;33mEmbedding\u001b[0m)          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (\u001b[38;5;33mGRU\u001b[0m)                            â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ layer_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "vqw06y-R0K9m"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv-Eutpy0K9n"
      },
      "source": [
        "Compile the Keras model so it is ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "GU8BzeXU0K9n"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "UD04S2t80K9o",
        "outputId": "1c48278f-680d-46dc-a850-3cf9f9922058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ layer_embedding (\u001b[38;5;33mEmbedding\u001b[0m)          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (\u001b[38;5;33mGRU\u001b[0m)                            â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ layer_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cU-duQL0K9o"
      },
      "source": [
        "## Train the Recurrent Neural Network\n",
        "\n",
        "We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "scrolled": true,
        "id": "fyh9FpCb0K9o",
        "outputId": "82bc3ae4-d7eb-4bc5-a480-dd00dd98df4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m372/372\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 766ms/step - accuracy: 0.6442 - loss: 0.6015 - val_accuracy: 0.8288 - val_loss: 0.3955\n",
            "Epoch 2/3\n",
            "\u001b[1m372/372\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 766ms/step - accuracy: 0.8877 - loss: 0.2972 - val_accuracy: 0.8464 - val_loss: 0.3845\n",
            "Epoch 3/3\n",
            "\u001b[1m372/372\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 772ms/step - accuracy: 0.9121 - loss: 0.2409 - val_accuracy: 0.8496 - val_loss: 0.3649\n",
            "CPU times: user 22min 28s, sys: 23.5 s, total: 22min 51s\n",
            "Wall time: 16min 14s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a034eebb520>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "%%time\n",
        "model.fit(x_train_pad, y_train,\n",
        "          validation_split=0.05, epochs=3, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5PO3jTu0K9o"
      },
      "source": [
        "## Performance on Test-Set\n",
        "\n",
        "Now that the model has been trained we can calculate its classification accuracy on the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf57VXFo0K9p",
        "outputId": "06aab747-762a-4686-ca06-769be87597ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 12s 493us/sample - loss: 0.3331 - accuracy: 0.8674\n",
            "CPU times: user 14 s, sys: 404 ms, total: 14.4 s\n",
            "Wall time: 12.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "result = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7T2LDjIL0K9p",
        "outputId": "03c50ad8-7066-4536-ac37-4ca131b9cb7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 86.74%\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy: {0:.2%}\".format(result[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LhgnbYJ0K9p"
      },
      "source": [
        "## Example of Mis-Classified Text\n",
        "\n",
        "In order to show an example of mis-classified text, we first calculate the predicted sentiment for the first 1000 texts in the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drfPm6U-0K9p",
        "outputId": "2788a8e7-4c6b-4209-f1d2-e8abc3deb94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.08 s, sys: 23.5 ms, total: 1.1 s\n",
            "Wall time: 1.03 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "y_pred = model.predict(x=x_test_pad[0:1000])\n",
        "y_pred = y_pred.T[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gSfTvzk0K9q"
      },
      "source": [
        "These predicted numbers fall between 0.0 and 1.0. We use a cutoff / threshold and say that all values above 0.5 are taken to be 1.0 and all values below 0.5 are taken to be 0.0. This gives us a predicted \"class\" of either 0.0 or 1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vPoaGtY0K9q"
      },
      "outputs": [],
      "source": [
        "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbspTO4u0K9q"
      },
      "source": [
        "The true \"class\" for the first 1000 texts in the test-set are needed for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPODRUVC0K9q"
      },
      "outputs": [],
      "source": [
        "cls_true = np.array(y_test[0:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ArGF1zK0K9q"
      },
      "source": [
        "We can then get indices for all the texts that were incorrectly classified by comparing all the \"classes\" of these two arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbwUlnl60K9r"
      },
      "outputs": [],
      "source": [
        "incorrect = np.where(cls_pred != cls_true)\n",
        "incorrect = incorrect[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDNS3_qy0K9r"
      },
      "source": [
        "Of the 1000 texts used, how many were mis-classified?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6bp-1WE0K9r",
        "outputId": "0f85d451-9d54-4aed-9878-3b648491c1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(incorrect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfXvPFOD0K9r"
      },
      "source": [
        "Let us look at the first mis-classified text. We will use its index several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c8JTWI60K9r",
        "outputId": "93bf18f5-ea95-420a-abd7-1da3bfc3e1b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx = incorrect[0]\n",
        "idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwX1rR530K9s"
      },
      "source": [
        "The mis-classified text is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "vPiF-Jbl0K9s",
        "outputId": "72372635-c04e-434e-ad67-0ad547e4add8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not dict",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-4636f0ca1b1c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not dict"
          ]
        }
      ],
      "source": [
        "text = x_test_text[idx]\n",
        "text[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHoacjUX0K9s"
      },
      "source": [
        "These are the predicted and true classes for the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1H3_ahH0K9s",
        "outputId": "98699e84-180c-4853-d146-508f19ad2934"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.27373913"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUkwgeS_0K9t",
        "outputId": "f9b4b1b6-d284-4b46-ac88-ec603da516e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cls_true[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv2bI8Ih0K9t"
      },
      "source": [
        "## New Data\n",
        "\n",
        "Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctr8Z0gG0K9t"
      },
      "outputs": [],
      "source": [
        "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "text2 = \"Good movie!\"\n",
        "text3 = \"Maybe I like this movie.\"\n",
        "text4 = \"Meh ...\"\n",
        "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
        "text6 = \"Bad movie!\"\n",
        "text7 = \"Not a good movie!\"\n",
        "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StYz5Fw60K9t"
      },
      "source": [
        "We first convert these texts to arrays of integer-tokens because that is needed by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XNpOwZr0K9u"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.texts_to_sequences(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vvhYb3a0K9u"
      },
      "source": [
        "To input texts with different lengths into the model, we also need to pad and truncate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMirhteI0K9u",
        "outputId": "7ab15c60-82c4-4e2a-a152-b87799e77137"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 544)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
        "                           padding=pad, truncating=pad)\n",
        "tokens_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2P_cURo0K9u"
      },
      "source": [
        "We can now use the trained model to predict the sentiment for these texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TAHh8k10K9u",
        "outputId": "4114f82e-e7d3-4c30-bd7d-378c14dee9fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.95301837],\n",
              "       [0.92733926],\n",
              "       [0.79257476],\n",
              "       [0.9019553 ],\n",
              "       [0.5875022 ],\n",
              "       [0.55110747],\n",
              "       [0.89896274],\n",
              "       [0.33616564]], dtype=float32)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(tokens_pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuoXEvMM0K9v"
      },
      "source": [
        "A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRAVEVuv0K9v"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
        "\n",
        "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
        "\n",
        "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n",
        "\n",
        "First we need to get the embedding-layer from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0imGUk_u0K9v"
      },
      "outputs": [],
      "source": [
        "layer_embedding = model.get_layer('layer_embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7z17tWW0K9v"
      },
      "source": [
        "We can then get the weights used for the mapping done by the embedding-layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZNIf3oF0K9v"
      },
      "outputs": [],
      "source": [
        "weights_embedding = layer_embedding.get_weights()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuBaC_7g0K9w"
      },
      "source": [
        "Note that the weights are actually just a matrix with the number of words in the vocabulary times the vector length for each embedding. That's because it is basically just a lookup-matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x34as_WT0K9w",
        "outputId": "dafe5c9f-3aeb-47b8-c622-265abbee9670"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 8)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJH9G24E0K9w"
      },
      "source": [
        "Let us get the integer-token for the word 'good', which is just an index into the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDo8j7l00K9w",
        "outputId": "cc1db55e-7bb2-442a-d7ab-27551bba1156"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_good = tokenizer.word_index['good']\n",
        "token_good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4TbqJVY0K9w"
      },
      "source": [
        "Let us also get the integer-token for the word 'great'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-waT-I0G0K9x",
        "outputId": "ea7b7925-df76-4a05-b9b4-d2b446cff747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_great = tokenizer.word_index['great']\n",
        "token_great"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8EVwTDa0K9x"
      },
      "source": [
        "These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n",
        "\n",
        "Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKxTvQbL0K9x",
        "outputId": "8cdf774d-d6fe-47da-d64e-062140f16ad4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.01839033,  0.05229224,  0.0848575 ,  0.03222338, -0.03947427,\n",
              "       -0.03776564, -0.01149088, -0.07443853], dtype=float32)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_embedding[token_good]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMPXD_ic0K9x",
        "outputId": "e71e5d2a-d453-422a-ad7f-48b76bbaefad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.14307617,  0.08333486,  0.15650608,  0.08930028, -0.08659173,\n",
              "       -0.12289459, -0.14367667, -0.10402057], dtype=float32)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_embedding[token_great]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wdnRrIv0K9x"
      },
      "source": [
        "Similarly, we can compare the embeddings for the words 'bad' and 'horrible'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr9m1B700K9y"
      },
      "outputs": [],
      "source": [
        "token_bad = tokenizer.word_index['bad']\n",
        "token_horrible = tokenizer.word_index['horrible']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdSiENay0K9y",
        "outputId": "ba106374-e0d5-4176-8ed5-4b9ac690ee2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.05553182, -0.09014519, -0.06248455, -0.11525143,  0.14601274,\n",
              "        0.07451952,  0.10784499,  0.10799433], dtype=float32)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_embedding[token_bad]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dynqq2K20K9y",
        "outputId": "d4907492-ad90-4298-fd07-12d7e4317a91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.15100664, -0.13359004, -0.15154287, -0.12776676,  0.10830297,\n",
              "        0.15224072,  0.13508266,  0.14284784], dtype=float32)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_embedding[token_horrible]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPmLOv450K9y"
      },
      "source": [
        "### Sorted Words\n",
        "\n",
        "We can also sort all the words in the vocabulary according to their \"similarity\" in the embedding-space. We want to see if words that have similar embedding-vectors also have similar meanings.\n",
        "\n",
        "Similarity of embedding-vectors can be measured by different metrics, e.g. Euclidean distance or cosine distance.\n",
        "\n",
        "We have a helper-function for calculating these distances and printing the words in sorted order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "halCoFSO0K9y"
      },
      "outputs": [],
      "source": [
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = weights_embedding[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(weights_embedding, [embedding],\n",
        "                      metric=metric).T[0]\n",
        "\n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "\n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "\n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [inverse_map[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for word, distance in zip(words, distances):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ3e5ksh0K9z"
      },
      "source": [
        "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "MRBr8Axo0K9z",
        "outputId": "713244a4-c884-4af4-f15d-c208512bcd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.012 - spring\n",
            "0.013 - 1980\n",
            "0.013 - permanent\n",
            "0.013 - robinson\n",
            "0.015 - anime\n",
            "0.015 - pleasantly\n",
            "0.016 - inter\n",
            "0.016 - profit\n",
            "0.017 - ramones\n",
            "...\n",
            "1.988 - mst3k\n",
            "1.988 - consist\n",
            "1.988 - save\n",
            "1.989 - unless\n",
            "1.990 - ripoff\n",
            "1.991 - insipid\n",
            "1.994 - avoid\n",
            "1.995 - drivel\n",
            "1.995 - expand\n",
            "1.995 - profile\n"
          ]
        }
      ],
      "source": [
        "print_sorted_words('great', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx2_rPyP0K9z"
      },
      "source": [
        "Similarly, we can print the words that are near and far from the word 'worst' in terms of their vector-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "npuIEgal0K9z",
        "outputId": "e38dc76f-fe17-4258-9d0f-b350b6cd42cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distance from 'worst':\n",
            "0.000 - worst\n",
            "0.004 - horrible\n",
            "0.004 - dull\n",
            "0.005 - below\n",
            "0.005 - boredom\n",
            "0.006 - conceived\n",
            "0.008 - salvage\n",
            "0.009 - slapped\n",
            "0.009 - fails\n",
            "0.010 - virus\n",
            "...\n",
            "1.989 - cried\n",
            "1.989 - compelling\n",
            "1.990 - carell\n",
            "1.990 - stadium\n",
            "1.991 - deanna\n",
            "1.992 - eddie\n",
            "1.992 - resolved\n",
            "1.992 - sirk\n",
            "1.994 - sidney\n",
            "1.997 - concentrates\n"
          ]
        }
      ],
      "source": [
        "print_sorted_words('worst', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc5N_umr0K9z"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This tutorial showed the basic methods for doing Natural Language Processing (NLP) using a Recurrent Neural Network with integer-tokens and an embedding layer. This was used to do sentiment analysis of movie reviews from IMDB. It works reasonably well if the hyper-parameters are chosen properly. But it is important to understand that this is not human-like comprehension of text. The system does not have any real understanding of the text. It is just a clever way of doing pattern-recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmRUd8nC0K90"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "These are a few suggestions for exercises that may help improve your skills with TensorFlow. It is important to get hands-on experience with TensorFlow in order to learn how to use it properly.\n",
        "\n",
        "You may want to backup this Notebook before making any changes.\n",
        "\n",
        "* Run more training-epochs. Does it improve performance?\n",
        "* If your model overfits the training-data, try using dropout-layers and dropout inside the GRU.\n",
        "* Increase or decrease the number of words in the vocabulary. This is done when the `Tokenizer` is initialized. Does it affect performance?\n",
        "* Increase the size of the embedding-vectors to e.g. 200. Does it affect performance?\n",
        "* Try varying all the different hyper-parameters for the Recurrent Neural Network.\n",
        "* Use Bayesian Optimization from Tutorial #19 to find the best choice of hyper-parameters.\n",
        "* Use 'post' for padding and truncating in `pad_sequences()`. Does it affect the performance?\n",
        "* Use individual characters instead of tokenized words as the vocabulary. You can then use one-hot encoded vectors for each character instead of using the embedding-layer.\n",
        "* Use `model.fit_generator()` instead of `model.fit()` and make your own data-generator, which creates a batch of data using a random subset of `x_train_tokens`. The sequences must be padded so they all match the length of the longest sequence.\n",
        "* Explain to a friend how the program works."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
